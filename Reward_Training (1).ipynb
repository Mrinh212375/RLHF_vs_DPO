{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64873e32-8588-43c0-b881-8571e8493030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: trl in /home/azureuser/.local/lib/python3.10/site-packages (0.7.11)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /home/azureuser/.local/lib/python3.10/site-packages (from trl) (0.5.11)\n",
      "Requirement already satisfied: accelerate in /home/azureuser/.local/lib/python3.10/site-packages (from trl) (0.26.1)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /home/azureuser/.local/lib/python3.10/site-packages (from trl) (4.37.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /home/azureuser/.local/lib/python3.10/site-packages (from trl) (1.26.1)\n",
      "Requirement already satisfied: datasets in /home/azureuser/.local/lib/python3.10/site-packages (from trl) (2.16.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/azureuser/.local/lib/python3.10/site-packages (from trl) (2.1.2)\n",
      "Requirement already satisfied: fsspec in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (10.3.2.106)\n",
      "Requirement already satisfied: typing-extensions in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.8.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2.1.0)\n",
      "Requirement already satisfied: networkx in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: filelock in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.13.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.0.2.54)\n",
      "Requirement already satisfied: jinja2 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.4.5.107)\n",
      "Requirement already satisfied: sympy in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/azureuser/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.3.52)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/azureuser/.local/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers>=4.31.0->trl) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/azureuser/.local/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/azureuser/.local/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.15.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/azureuser/.local/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (23.2)\n",
      "Requirement already satisfied: requests in /home/azureuser/.local/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/azureuser/.local/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (4.66.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/azureuser/.local/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.4.2)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /home/azureuser/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/azureuser/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.6.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/azureuser/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (1.6.4)\n",
      "Requirement already satisfied: psutil in /home/azureuser/.local/lib/python3.10/site-packages (from accelerate->trl) (5.9.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/azureuser/.local/lib/python3.10/site-packages (from datasets->trl) (13.0.0)\n",
      "Requirement already satisfied: xxhash in /home/azureuser/.local/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/azureuser/.local/lib/python3.10/site-packages (from datasets->trl) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /home/azureuser/.local/lib/python3.10/site-packages (from datasets->trl) (3.8.6)\n",
      "Requirement already satisfied: pandas in /home/azureuser/.local/lib/python3.10/site-packages (from datasets->trl) (2.1.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/azureuser/.local/lib/python3.10/site-packages (from datasets->trl) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/azureuser/.local/lib/python3.10/site-packages (from datasets->trl) (0.3.7)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/azureuser/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/azureuser/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/azureuser/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (3.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/azureuser/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/azureuser/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/azureuser/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/azureuser/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers>=4.31.0->trl) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers>=4.31.0->trl) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/azureuser/.local/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2023.11.17)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/azureuser/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/azureuser/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/azureuser/.local/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/azureuser/.local/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/azureuser/.local/lib/python3.10/site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets->trl) (2022.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/azureuser/.local/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/azureuser/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /home/azureuser/.local/lib/python3.10/site-packages (0.26.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/azureuser/.local/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /home/azureuser/.local/lib/python3.10/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/azureuser/.local/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/azureuser/.local/lib/python3.10/site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/azureuser/.local/lib/python3.10/site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/azureuser/.local/lib/python3.10/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: sympy in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: fsspec in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: networkx in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/azureuser/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/azureuser/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.52)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/azureuser/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: requests in /home/azureuser/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/azureuser/.local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/azureuser/.local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/azureuser/.local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/azureuser/.local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.26.1\n",
      "    Uninstalling accelerate-0.26.1:\n",
      "      Successfully uninstalled accelerate-0.26.1\n",
      "Successfully installed accelerate-0.27.2\n"
     ]
    }
   ],
   "source": [
    "!pip install trl\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c454d382-f6b4-409d-9efe-b1126c1ba079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:03:05.065942: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-12 15:03:05.065988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-12 15:03:05.067154: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-12 15:03:05.074233: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-12 15:03:05.824224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, BitsAndBytesConfig,AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "from trl import RewardTrainer, SFTTrainer , AutoModelForSeq2SeqLMWithValueHead,PPOTrainer, PPOConfig\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "from datasets import Dataset, load_dataset\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "366e2adf-b15a-4b89-b78f-89e1ff9d0731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load a tokenizer (change the model name as per your requirements)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43814597-5ca4-4b16-aa21-c31424449fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('small_Rlhf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4895e35-50b9-4ab0-bcf3-88a7615decc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUBREDDIT: r/relationship_advice\\nTITLE: [19/M...</td>\n",
       "      <td>TL;DR: Girlfriend thinks she's holding me back...</td>\n",
       "      <td>TL;DR: Girlfriend wants me to experience colle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUBREDDIT: r/tifu\\nTITLE: TIFU by making a pro...</td>\n",
       "      <td>TL;DR: I said a prostitute joke to my friend a...</td>\n",
       "      <td>TL;DR: TIFU by making a prostitute joke</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  SUBREDDIT: r/relationship_advice\\nTITLE: [19/M...   \n",
       "1  SUBREDDIT: r/tifu\\nTITLE: TIFU by making a pro...   \n",
       "\n",
       "                                              chosen  \\\n",
       "0  TL;DR: Girlfriend thinks she's holding me back...   \n",
       "1  TL;DR: I said a prostitute joke to my friend a...   \n",
       "\n",
       "                                            rejected  \n",
       "0  TL;DR: Girlfriend wants me to experience colle...  \n",
       "1            TL;DR: TIFU by making a prostitute joke  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09eb47c4-3dea-4690-9b52-dd52a840205e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6931589f-6f71-4c64-8256-9ac3820dbc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = Dataset.from_pandas(df)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0fa4aff-be21-4aa6-92fc-56e603e1d1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8869220a8c824b90aa78219d13ae0d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token count: 116\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset and get the lengths\n",
    "tokenized_lengths = raw_dataset.map(lambda examples: {'lengths': len(tokenizer(examples['chosen'], add_special_tokens=True)[\"input_ids\"])}, remove_columns=raw_dataset.column_names)\n",
    "# Fetch max length\n",
    "max_length = max(tokenized_lengths[\"lengths\"])\n",
    "print(\"Max token count:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "535b7e4b-37b7-4366-aeba-f22c680ff2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d09886ff-7377-43eb-83c3-beb71f081844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe411651-9478-4ef0-b278-69f645c5d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, val, and test\n",
    "train_percent = 0.8\n",
    "val_percent = 0.1\n",
    "# test_percent is implicitly 0.1 since train + val + test = 1.0\n",
    "\n",
    "train_size = int(train_percent * 1000)\n",
    "val_size = int(val_percent * 1000)\n",
    "\n",
    "# Remaining samples are for testing\n",
    "train_dataset = raw_dataset.select(list(range(train_size)))\n",
    "val_dataset = raw_dataset.select(list(range(train_size, train_size + val_size)))\n",
    "test_dataset = raw_dataset.select(list(range(train_size + val_size, 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7823e82-0092-44e7-b21e-bc2cec389b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 800\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset ,val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663295ec-394c-4607-93a7-9b7ac83bebd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c75af8e7d8447c9c73ff8a5825cdb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becbc77325ad45b0a55f7b9e36642fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_features(batch):\n",
    "    # Tokenize 'chosen' feature\n",
    "    chosen_tokens = tokenizer(batch['chosen'], padding='max_length', truncation=True, max_length=512, return_tensors='np')\n",
    "    batch['input_ids_chosen'] = chosen_tokens['input_ids']\n",
    "    batch['attention_mask_chosen'] = chosen_tokens['attention_mask']\n",
    "\n",
    "    # Tokenize 'rejected' feature\n",
    "    rejected_tokens = tokenizer(batch['rejected'], padding='max_length', truncation=True, max_length=512, return_tensors='np')\n",
    "    batch['input_ids_rejected'] = rejected_tokens['input_ids']\n",
    "    batch['attention_mask_rejected'] = rejected_tokens['attention_mask']\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Apply the function to your datasets\n",
    "train_dataset = train_dataset.map(process_features, batched=True)\n",
    "val_dataset = val_dataset.map(process_features, batched=True)\n",
    "\n",
    "# Remove original 'chosen' and 'rejected' columns\n",
    "columns_to_remove = ['chosen', 'rejected']\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "val_dataset = val_dataset.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf09314e-5e8a-4f00-a59d-e93aa466ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14099/712054621.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b88dca6d-6f7a-4ff5-82f7-a8a49923ed05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.10/site-packages/trl/trainer/reward_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/trl/trainer/reward_trainer.py:115: FutureWarning: The `max_length` argument is deprecated and will be removed in a future version. Please use the `RewardConfig` to set `max_length` instead.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/trl/trainer/reward_trainer.py:189: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'rlhf_reward_model/'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=50,\n",
    "    logging_steps = 1\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    #peft_config=peft_config,\n",
    "    compute_metrics=compute_metrics,\n",
    "    max_length=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a48e274-8f30-41c5-a333-d54422d45d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 12 15:03:09 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   42C    P0              26W /  70W |   2497MiB / 16384MiB |     22%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4031      C   /usr/bin/python3                           1920MiB |\n",
      "|    0   N/A  N/A     14099      C   /usr/bin/python3                            572MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5273ffb2-ad34-4a38-b594-73b53c8e0184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 1:59:13, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.762900</td>\n",
       "      <td>0.672661</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.759062</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.216400</td>\n",
       "      <td>1.104303</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>1.203124</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>1.325342</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.863919</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>2.200639</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.718042</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>1.534842</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>2.191980</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.663610</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>1.451328</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.556268</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.951545</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.058141</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.349641</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>2.003707</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.871887</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.913801</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.837395</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.156133</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.075900</td>\n",
       "      <td>2.183444</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>2.026271</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>2.312790</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.427121</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.279430</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.190053</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.268351</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.167700</td>\n",
       "      <td>2.181820</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.401589</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>2.434116</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.346214</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.159670</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.352508</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.220518</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.349889</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.251884</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.269852</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.239331</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.400676</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.459647</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.488572</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.372595</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.426684</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>2.497152</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.514446</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>2.481951</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.434191</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.438241</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.442726</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory rlhf_reward_model/checkpoint-100 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.0709556962271412, metrics={'train_runtime': 7155.2215, 'train_samples_per_second': 5.59, 'train_steps_per_second': 0.699, 'total_flos': 0.0, 'train_loss': 0.0709556962271412, 'epoch': 50.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "222e16d5-9d6c-405b-8e80-b51131d740ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 0.684, 'learning_rate': 4.999e-05, 'epoch': 0.01, 'step': 1},\n",
       " {'loss': 0.6819,\n",
       "  'learning_rate': 4.9980000000000006e-05,\n",
       "  'epoch': 0.02,\n",
       "  'step': 2},\n",
       " {'loss': 0.7034, 'learning_rate': 4.997e-05, 'epoch': 0.03, 'step': 3},\n",
       " {'loss': 0.6787, 'learning_rate': 4.996e-05, 'epoch': 0.04, 'step': 4},\n",
       " {'loss': 0.7213, 'learning_rate': 4.995e-05, 'epoch': 0.05, 'step': 5},\n",
       " {'loss': 0.6909,\n",
       "  'learning_rate': 4.9940000000000006e-05,\n",
       "  'epoch': 0.06,\n",
       "  'step': 6},\n",
       " {'loss': 0.6958,\n",
       "  'learning_rate': 4.9930000000000005e-05,\n",
       "  'epoch': 0.07,\n",
       "  'step': 7},\n",
       " {'loss': 0.6489, 'learning_rate': 4.992e-05, 'epoch': 0.08, 'step': 8},\n",
       " {'loss': 0.7301, 'learning_rate': 4.991e-05, 'epoch': 0.09, 'step': 9},\n",
       " {'loss': 0.7351, 'learning_rate': 4.99e-05, 'epoch': 0.1, 'step': 10},\n",
       " {'loss': 0.6535,\n",
       "  'learning_rate': 4.9890000000000005e-05,\n",
       "  'epoch': 0.11,\n",
       "  'step': 11},\n",
       " {'loss': 0.7034,\n",
       "  'learning_rate': 4.9880000000000004e-05,\n",
       "  'epoch': 0.12,\n",
       "  'step': 12},\n",
       " {'loss': 0.6867, 'learning_rate': 4.987e-05, 'epoch': 0.13, 'step': 13},\n",
       " {'loss': 0.7132, 'learning_rate': 4.986e-05, 'epoch': 0.14, 'step': 14},\n",
       " {'loss': 0.6857,\n",
       "  'learning_rate': 4.9850000000000006e-05,\n",
       "  'epoch': 0.15,\n",
       "  'step': 15},\n",
       " {'loss': 0.6639,\n",
       "  'learning_rate': 4.9840000000000004e-05,\n",
       "  'epoch': 0.16,\n",
       "  'step': 16},\n",
       " {'loss': 0.7316, 'learning_rate': 4.983e-05, 'epoch': 0.17, 'step': 17},\n",
       " {'loss': 0.6532, 'learning_rate': 4.982e-05, 'epoch': 0.18, 'step': 18},\n",
       " {'loss': 0.7221, 'learning_rate': 4.981e-05, 'epoch': 0.19, 'step': 19},\n",
       " {'loss': 0.7141,\n",
       "  'learning_rate': 4.9800000000000004e-05,\n",
       "  'epoch': 0.2,\n",
       "  'step': 20},\n",
       " {'loss': 0.7701, 'learning_rate': 4.979e-05, 'epoch': 0.21, 'step': 21},\n",
       " {'loss': 0.6645, 'learning_rate': 4.978e-05, 'epoch': 0.22, 'step': 22},\n",
       " {'loss': 0.6279, 'learning_rate': 4.977e-05, 'epoch': 0.23, 'step': 23},\n",
       " {'loss': 0.655, 'learning_rate': 4.976e-05, 'epoch': 0.24, 'step': 24},\n",
       " {'loss': 0.7792, 'learning_rate': 4.975e-05, 'epoch': 0.25, 'step': 25},\n",
       " {'loss': 0.6704, 'learning_rate': 4.974e-05, 'epoch': 0.26, 'step': 26},\n",
       " {'loss': 0.71,\n",
       "  'learning_rate': 4.973000000000001e-05,\n",
       "  'epoch': 0.27,\n",
       "  'step': 27},\n",
       " {'loss': 0.7043, 'learning_rate': 4.972e-05, 'epoch': 0.28, 'step': 28},\n",
       " {'loss': 0.7298,\n",
       "  'learning_rate': 4.9710000000000003e-05,\n",
       "  'epoch': 0.29,\n",
       "  'step': 29},\n",
       " {'loss': 0.6902, 'learning_rate': 4.97e-05, 'epoch': 0.3, 'step': 30},\n",
       " {'loss': 0.7213, 'learning_rate': 4.969e-05, 'epoch': 0.31, 'step': 31},\n",
       " {'loss': 0.7091,\n",
       "  'learning_rate': 4.9680000000000005e-05,\n",
       "  'epoch': 0.32,\n",
       "  'step': 32},\n",
       " {'loss': 0.7023, 'learning_rate': 4.967e-05, 'epoch': 0.33, 'step': 33},\n",
       " {'loss': 0.6833, 'learning_rate': 4.966e-05, 'epoch': 0.34, 'step': 34},\n",
       " {'loss': 0.7082, 'learning_rate': 4.965e-05, 'epoch': 0.35, 'step': 35},\n",
       " {'loss': 0.7103,\n",
       "  'learning_rate': 4.9640000000000006e-05,\n",
       "  'epoch': 0.36,\n",
       "  'step': 36},\n",
       " {'loss': 0.681,\n",
       "  'learning_rate': 4.9630000000000004e-05,\n",
       "  'epoch': 0.37,\n",
       "  'step': 37},\n",
       " {'loss': 0.6623, 'learning_rate': 4.962e-05, 'epoch': 0.38, 'step': 38},\n",
       " {'loss': 0.6399, 'learning_rate': 4.961e-05, 'epoch': 0.39, 'step': 39},\n",
       " {'loss': 0.7442, 'learning_rate': 4.96e-05, 'epoch': 0.4, 'step': 40},\n",
       " {'loss': 0.7079,\n",
       "  'learning_rate': 4.9590000000000005e-05,\n",
       "  'epoch': 0.41,\n",
       "  'step': 41},\n",
       " {'loss': 0.6283, 'learning_rate': 4.958e-05, 'epoch': 0.42, 'step': 42},\n",
       " {'loss': 0.7308, 'learning_rate': 4.957e-05, 'epoch': 0.43, 'step': 43},\n",
       " {'loss': 0.6983, 'learning_rate': 4.956e-05, 'epoch': 0.44, 'step': 44},\n",
       " {'loss': 0.7219,\n",
       "  'learning_rate': 4.9550000000000005e-05,\n",
       "  'epoch': 0.45,\n",
       "  'step': 45},\n",
       " {'loss': 0.6674,\n",
       "  'learning_rate': 4.9540000000000003e-05,\n",
       "  'epoch': 0.46,\n",
       "  'step': 46},\n",
       " {'loss': 0.6611, 'learning_rate': 4.953e-05, 'epoch': 0.47, 'step': 47},\n",
       " {'loss': 0.666, 'learning_rate': 4.952e-05, 'epoch': 0.48, 'step': 48},\n",
       " {'loss': 0.6414, 'learning_rate': 4.951e-05, 'epoch': 0.49, 'step': 49},\n",
       " {'loss': 0.6858,\n",
       "  'learning_rate': 4.9500000000000004e-05,\n",
       "  'epoch': 0.5,\n",
       "  'step': 50},\n",
       " {'loss': 0.6257, 'learning_rate': 4.949e-05, 'epoch': 0.51, 'step': 51},\n",
       " {'loss': 0.6617,\n",
       "  'learning_rate': 4.948000000000001e-05,\n",
       "  'epoch': 0.52,\n",
       "  'step': 52},\n",
       " {'loss': 0.6955, 'learning_rate': 4.947e-05, 'epoch': 0.53, 'step': 53},\n",
       " {'loss': 0.6807, 'learning_rate': 4.946e-05, 'epoch': 0.54, 'step': 54},\n",
       " {'loss': 0.7425, 'learning_rate': 4.945e-05, 'epoch': 0.55, 'step': 55},\n",
       " {'loss': 0.6321, 'learning_rate': 4.944e-05, 'epoch': 0.56, 'step': 56},\n",
       " {'loss': 0.69,\n",
       "  'learning_rate': 4.9430000000000006e-05,\n",
       "  'epoch': 0.57,\n",
       "  'step': 57},\n",
       " {'loss': 0.6296, 'learning_rate': 4.942e-05, 'epoch': 0.58, 'step': 58},\n",
       " {'loss': 0.674, 'learning_rate': 4.941e-05, 'epoch': 0.59, 'step': 59},\n",
       " {'loss': 0.7877, 'learning_rate': 4.94e-05, 'epoch': 0.6, 'step': 60},\n",
       " {'loss': 0.7128, 'learning_rate': 4.939e-05, 'epoch': 0.61, 'step': 61},\n",
       " {'loss': 0.6448,\n",
       "  'learning_rate': 4.9380000000000005e-05,\n",
       "  'epoch': 0.62,\n",
       "  'step': 62},\n",
       " {'loss': 0.6468, 'learning_rate': 4.937e-05, 'epoch': 0.63, 'step': 63},\n",
       " {'loss': 0.6812, 'learning_rate': 4.936e-05, 'epoch': 0.64, 'step': 64},\n",
       " {'loss': 0.6538, 'learning_rate': 4.935e-05, 'epoch': 0.65, 'step': 65},\n",
       " {'loss': 0.674,\n",
       "  'learning_rate': 4.9340000000000005e-05,\n",
       "  'epoch': 0.66,\n",
       "  'step': 66},\n",
       " {'loss': 0.7194,\n",
       "  'learning_rate': 4.9330000000000004e-05,\n",
       "  'epoch': 0.67,\n",
       "  'step': 67},\n",
       " {'loss': 0.643, 'learning_rate': 4.932e-05, 'epoch': 0.68, 'step': 68},\n",
       " {'loss': 0.7101, 'learning_rate': 4.931e-05, 'epoch': 0.69, 'step': 69},\n",
       " {'loss': 0.7555, 'learning_rate': 4.93e-05, 'epoch': 0.7, 'step': 70},\n",
       " {'loss': 0.7299,\n",
       "  'learning_rate': 4.9290000000000004e-05,\n",
       "  'epoch': 0.71,\n",
       "  'step': 71},\n",
       " {'loss': 0.6975, 'learning_rate': 4.928e-05, 'epoch': 0.72, 'step': 72},\n",
       " {'loss': 0.7799,\n",
       "  'learning_rate': 4.927000000000001e-05,\n",
       "  'epoch': 0.73,\n",
       "  'step': 73},\n",
       " {'loss': 0.806, 'learning_rate': 4.926e-05, 'epoch': 0.74, 'step': 74},\n",
       " {'loss': 0.6884,\n",
       "  'learning_rate': 4.9250000000000004e-05,\n",
       "  'epoch': 0.75,\n",
       "  'step': 75},\n",
       " {'loss': 0.6875, 'learning_rate': 4.924e-05, 'epoch': 0.76, 'step': 76},\n",
       " {'loss': 0.6976, 'learning_rate': 4.923e-05, 'epoch': 0.77, 'step': 77},\n",
       " {'loss': 0.7043,\n",
       "  'learning_rate': 4.9220000000000006e-05,\n",
       "  'epoch': 0.78,\n",
       "  'step': 78},\n",
       " {'loss': 0.5806, 'learning_rate': 4.921e-05, 'epoch': 0.79, 'step': 79},\n",
       " {'loss': 0.6253, 'learning_rate': 4.92e-05, 'epoch': 0.8, 'step': 80},\n",
       " {'loss': 0.597, 'learning_rate': 4.919e-05, 'epoch': 0.81, 'step': 81},\n",
       " {'loss': 0.6497,\n",
       "  'learning_rate': 4.918000000000001e-05,\n",
       "  'epoch': 0.82,\n",
       "  'step': 82},\n",
       " {'loss': 0.7675,\n",
       "  'learning_rate': 4.9170000000000005e-05,\n",
       "  'epoch': 0.83,\n",
       "  'step': 83},\n",
       " {'loss': 0.6804,\n",
       "  'learning_rate': 4.9160000000000004e-05,\n",
       "  'epoch': 0.84,\n",
       "  'step': 84},\n",
       " {'loss': 0.6578, 'learning_rate': 4.915e-05, 'epoch': 0.85, 'step': 85},\n",
       " {'loss': 0.6634, 'learning_rate': 4.914e-05, 'epoch': 0.86, 'step': 86},\n",
       " {'loss': 0.6985,\n",
       "  'learning_rate': 4.9130000000000006e-05,\n",
       "  'epoch': 0.87,\n",
       "  'step': 87},\n",
       " {'loss': 0.6371,\n",
       "  'learning_rate': 4.9120000000000004e-05,\n",
       "  'epoch': 0.88,\n",
       "  'step': 88},\n",
       " {'loss': 0.7997, 'learning_rate': 4.911e-05, 'epoch': 0.89, 'step': 89},\n",
       " {'loss': 0.6017, 'learning_rate': 4.91e-05, 'epoch': 0.9, 'step': 90},\n",
       " {'loss': 0.7302,\n",
       "  'learning_rate': 4.9090000000000006e-05,\n",
       "  'epoch': 0.91,\n",
       "  'step': 91},\n",
       " {'loss': 0.7657,\n",
       "  'learning_rate': 4.9080000000000004e-05,\n",
       "  'epoch': 0.92,\n",
       "  'step': 92},\n",
       " {'loss': 0.6517, 'learning_rate': 4.907e-05, 'epoch': 0.93, 'step': 93},\n",
       " {'loss': 0.7659, 'learning_rate': 4.906e-05, 'epoch': 0.94, 'step': 94},\n",
       " {'loss': 0.5901, 'learning_rate': 4.905e-05, 'epoch': 0.95, 'step': 95},\n",
       " {'loss': 0.6503,\n",
       "  'learning_rate': 4.9040000000000005e-05,\n",
       "  'epoch': 0.96,\n",
       "  'step': 96},\n",
       " {'loss': 0.6543, 'learning_rate': 4.903e-05, 'epoch': 0.97, 'step': 97},\n",
       " {'loss': 0.6814, 'learning_rate': 4.902e-05, 'epoch': 0.98, 'step': 98},\n",
       " {'loss': 0.6315, 'learning_rate': 4.901e-05, 'epoch': 0.99, 'step': 99},\n",
       " {'loss': 0.7629, 'learning_rate': 4.9e-05, 'epoch': 1.0, 'step': 100},\n",
       " {'eval_loss': 0.6726608872413635,\n",
       "  'eval_accuracy': 0.56,\n",
       "  'eval_runtime': 5.7894,\n",
       "  'eval_samples_per_second': 17.273,\n",
       "  'eval_steps_per_second': 2.245,\n",
       "  'epoch': 1.0,\n",
       "  'step': 100},\n",
       " {'loss': 0.4679,\n",
       "  'learning_rate': 4.8990000000000004e-05,\n",
       "  'epoch': 1.01,\n",
       "  'step': 101},\n",
       " {'loss': 0.5564, 'learning_rate': 4.898e-05, 'epoch': 1.02, 'step': 102},\n",
       " {'loss': 0.6485,\n",
       "  'learning_rate': 4.897000000000001e-05,\n",
       "  'epoch': 1.03,\n",
       "  'step': 103},\n",
       " {'loss': 0.479, 'learning_rate': 4.896e-05, 'epoch': 1.04, 'step': 104},\n",
       " {'loss': 0.4774,\n",
       "  'learning_rate': 4.8950000000000004e-05,\n",
       "  'epoch': 1.05,\n",
       "  'step': 105},\n",
       " {'loss': 0.5079, 'learning_rate': 4.894e-05, 'epoch': 1.06, 'step': 106},\n",
       " {'loss': 0.5798, 'learning_rate': 4.893e-05, 'epoch': 1.07, 'step': 107},\n",
       " {'loss': 0.4864,\n",
       "  'learning_rate': 4.8920000000000006e-05,\n",
       "  'epoch': 1.08,\n",
       "  'step': 108},\n",
       " {'loss': 0.8324, 'learning_rate': 4.891e-05, 'epoch': 1.09, 'step': 109},\n",
       " {'loss': 0.5486, 'learning_rate': 4.89e-05, 'epoch': 1.1, 'step': 110},\n",
       " {'loss': 0.5636, 'learning_rate': 4.889e-05, 'epoch': 1.11, 'step': 111},\n",
       " {'loss': 0.4806,\n",
       "  'learning_rate': 4.8880000000000006e-05,\n",
       "  'epoch': 1.12,\n",
       "  'step': 112},\n",
       " {'loss': 0.4598,\n",
       "  'learning_rate': 4.8870000000000005e-05,\n",
       "  'epoch': 1.13,\n",
       "  'step': 113},\n",
       " {'loss': 0.4649, 'learning_rate': 4.886e-05, 'epoch': 1.14, 'step': 114},\n",
       " {'loss': 0.4918, 'learning_rate': 4.885e-05, 'epoch': 1.15, 'step': 115},\n",
       " {'loss': 0.5836, 'learning_rate': 4.884e-05, 'epoch': 1.16, 'step': 116},\n",
       " {'loss': 0.5376,\n",
       "  'learning_rate': 4.8830000000000005e-05,\n",
       "  'epoch': 1.17,\n",
       "  'step': 117},\n",
       " {'loss': 0.5033,\n",
       "  'learning_rate': 4.8820000000000004e-05,\n",
       "  'epoch': 1.18,\n",
       "  'step': 118},\n",
       " {'loss': 0.5913, 'learning_rate': 4.881e-05, 'epoch': 1.19, 'step': 119},\n",
       " {'loss': 0.5164, 'learning_rate': 4.88e-05, 'epoch': 1.2, 'step': 120},\n",
       " {'loss': 0.5903,\n",
       "  'learning_rate': 4.8790000000000006e-05,\n",
       "  'epoch': 1.21,\n",
       "  'step': 121},\n",
       " {'loss': 0.6735,\n",
       "  'learning_rate': 4.8780000000000004e-05,\n",
       "  'epoch': 1.22,\n",
       "  'step': 122},\n",
       " {'loss': 0.604, 'learning_rate': 4.877e-05, 'epoch': 1.23, 'step': 123},\n",
       " {'loss': 0.59, 'learning_rate': 4.876e-05, 'epoch': 1.24, 'step': 124},\n",
       " {'loss': 0.5462, 'learning_rate': 4.875e-05, 'epoch': 1.25, 'step': 125},\n",
       " {'loss': 0.3045,\n",
       "  'learning_rate': 4.8740000000000004e-05,\n",
       "  'epoch': 1.26,\n",
       "  'step': 126},\n",
       " {'loss': 0.6933, 'learning_rate': 4.873e-05, 'epoch': 1.27, 'step': 127},\n",
       " {'loss': 0.418,\n",
       "  'learning_rate': 4.872000000000001e-05,\n",
       "  'epoch': 1.28,\n",
       "  'step': 128},\n",
       " {'loss': 0.3809, 'learning_rate': 4.871e-05, 'epoch': 1.29, 'step': 129},\n",
       " {'loss': 0.607, 'learning_rate': 4.87e-05, 'epoch': 1.3, 'step': 130},\n",
       " {'loss': 0.5204, 'learning_rate': 4.869e-05, 'epoch': 1.31, 'step': 131},\n",
       " {'loss': 0.8705, 'learning_rate': 4.868e-05, 'epoch': 1.32, 'step': 132},\n",
       " {'loss': 0.3792,\n",
       "  'learning_rate': 4.867000000000001e-05,\n",
       "  'epoch': 1.33,\n",
       "  'step': 133},\n",
       " {'loss': 0.3716, 'learning_rate': 4.866e-05, 'epoch': 1.34, 'step': 134},\n",
       " {'loss': 0.2399,\n",
       "  'learning_rate': 4.8650000000000003e-05,\n",
       "  'epoch': 1.35,\n",
       "  'step': 135},\n",
       " {'loss': 0.6157, 'learning_rate': 4.864e-05, 'epoch': 1.36, 'step': 136},\n",
       " {'loss': 0.5018, 'learning_rate': 4.863e-05, 'epoch': 1.37, 'step': 137},\n",
       " {'loss': 1.0092,\n",
       "  'learning_rate': 4.8620000000000005e-05,\n",
       "  'epoch': 1.38,\n",
       "  'step': 138},\n",
       " {'loss': 0.9681, 'learning_rate': 4.861e-05, 'epoch': 1.39, 'step': 139},\n",
       " {'loss': 0.5632, 'learning_rate': 4.86e-05, 'epoch': 1.4, 'step': 140},\n",
       " {'loss': 0.5691, 'learning_rate': 4.859e-05, 'epoch': 1.41, 'step': 141},\n",
       " {'loss': 0.4254,\n",
       "  'learning_rate': 4.8580000000000006e-05,\n",
       "  'epoch': 1.42,\n",
       "  'step': 142},\n",
       " {'loss': 0.4548,\n",
       "  'learning_rate': 4.8570000000000004e-05,\n",
       "  'epoch': 1.43,\n",
       "  'step': 143},\n",
       " {'loss': 0.4466, 'learning_rate': 4.856e-05, 'epoch': 1.44, 'step': 144},\n",
       " {'loss': 0.7051, 'learning_rate': 4.855e-05, 'epoch': 1.45, 'step': 145},\n",
       " {'loss': 0.5333, 'learning_rate': 4.854e-05, 'epoch': 1.46, 'step': 146},\n",
       " {'loss': 0.7259,\n",
       "  'learning_rate': 4.8530000000000005e-05,\n",
       "  'epoch': 1.47,\n",
       "  'step': 147},\n",
       " {'loss': 0.6823, 'learning_rate': 4.852e-05, 'epoch': 1.48, 'step': 148},\n",
       " {'loss': 0.5135, 'learning_rate': 4.851e-05, 'epoch': 1.49, 'step': 149},\n",
       " {'loss': 0.6393, 'learning_rate': 4.85e-05, 'epoch': 1.5, 'step': 150},\n",
       " {'loss': 0.6051,\n",
       "  'learning_rate': 4.8490000000000005e-05,\n",
       "  'epoch': 1.51,\n",
       "  'step': 151},\n",
       " {'loss': 0.6229,\n",
       "  'learning_rate': 4.8480000000000003e-05,\n",
       "  'epoch': 1.52,\n",
       "  'step': 152},\n",
       " {'loss': 0.676, 'learning_rate': 4.847e-05, 'epoch': 1.53, 'step': 153},\n",
       " {'loss': 0.5204, 'learning_rate': 4.846e-05, 'epoch': 1.54, 'step': 154},\n",
       " {'loss': 0.3809, 'learning_rate': 4.845e-05, 'epoch': 1.55, 'step': 155},\n",
       " {'loss': 0.5258,\n",
       "  'learning_rate': 4.8440000000000004e-05,\n",
       "  'epoch': 1.56,\n",
       "  'step': 156},\n",
       " {'loss': 0.5668, 'learning_rate': 4.843e-05, 'epoch': 1.57, 'step': 157},\n",
       " {'loss': 0.7581,\n",
       "  'learning_rate': 4.842000000000001e-05,\n",
       "  'epoch': 1.58,\n",
       "  'step': 158},\n",
       " {'loss': 0.5657, 'learning_rate': 4.841e-05, 'epoch': 1.59, 'step': 159},\n",
       " {'loss': 0.5502,\n",
       "  'learning_rate': 4.8400000000000004e-05,\n",
       "  'epoch': 1.6,\n",
       "  'step': 160},\n",
       " {'loss': 0.5809, 'learning_rate': 4.839e-05, 'epoch': 1.61, 'step': 161},\n",
       " {'loss': 0.5982, 'learning_rate': 4.838e-05, 'epoch': 1.62, 'step': 162},\n",
       " {'loss': 0.6499,\n",
       "  'learning_rate': 4.8370000000000006e-05,\n",
       "  'epoch': 1.63,\n",
       "  'step': 163},\n",
       " {'loss': 0.4501, 'learning_rate': 4.836e-05, 'epoch': 1.64, 'step': 164},\n",
       " {'loss': 0.5398, 'learning_rate': 4.835e-05, 'epoch': 1.65, 'step': 165},\n",
       " {'loss': 0.4747, 'learning_rate': 4.834e-05, 'epoch': 1.66, 'step': 166},\n",
       " {'loss': 0.4966, 'learning_rate': 4.833e-05, 'epoch': 1.67, 'step': 167},\n",
       " {'loss': 0.602,\n",
       "  'learning_rate': 4.8320000000000005e-05,\n",
       "  'epoch': 1.68,\n",
       "  'step': 168},\n",
       " {'loss': 0.4477,\n",
       "  'learning_rate': 4.8309999999999997e-05,\n",
       "  'epoch': 1.69,\n",
       "  'step': 169},\n",
       " {'loss': 0.5654, 'learning_rate': 4.83e-05, 'epoch': 1.7, 'step': 170},\n",
       " {'loss': 0.5459, 'learning_rate': 4.829e-05, 'epoch': 1.71, 'step': 171},\n",
       " {'loss': 0.6068,\n",
       "  'learning_rate': 4.8280000000000005e-05,\n",
       "  'epoch': 1.72,\n",
       "  'step': 172},\n",
       " {'loss': 0.5562,\n",
       "  'learning_rate': 4.8270000000000004e-05,\n",
       "  'epoch': 1.73,\n",
       "  'step': 173},\n",
       " {'loss': 0.6167, 'learning_rate': 4.826e-05, 'epoch': 1.74, 'step': 174},\n",
       " {'loss': 0.3487, 'learning_rate': 4.825e-05, 'epoch': 1.75, 'step': 175},\n",
       " {'loss': 0.6919, 'learning_rate': 4.824e-05, 'epoch': 1.76, 'step': 176},\n",
       " {'loss': 0.4368,\n",
       "  'learning_rate': 4.8230000000000004e-05,\n",
       "  'epoch': 1.77,\n",
       "  'step': 177},\n",
       " {'loss': 0.6353, 'learning_rate': 4.822e-05, 'epoch': 1.78, 'step': 178},\n",
       " {'loss': 0.5291, 'learning_rate': 4.821e-05, 'epoch': 1.79, 'step': 179},\n",
       " {'loss': 0.376, 'learning_rate': 4.82e-05, 'epoch': 1.8, 'step': 180},\n",
       " {'loss': 0.6046,\n",
       "  'learning_rate': 4.8190000000000004e-05,\n",
       "  'epoch': 1.81,\n",
       "  'step': 181},\n",
       " {'loss': 0.5942, 'learning_rate': 4.818e-05, 'epoch': 1.82, 'step': 182},\n",
       " {'loss': 0.6778, 'learning_rate': 4.817e-05, 'epoch': 1.83, 'step': 183},\n",
       " {'loss': 0.4753, 'learning_rate': 4.816e-05, 'epoch': 1.84, 'step': 184},\n",
       " {'loss': 0.4064, 'learning_rate': 4.815e-05, 'epoch': 1.85, 'step': 185},\n",
       " {'loss': 0.604, 'learning_rate': 4.814e-05, 'epoch': 1.86, 'step': 186},\n",
       " {'loss': 0.6388, 'learning_rate': 4.813e-05, 'epoch': 1.87, 'step': 187},\n",
       " {'loss': 0.6516,\n",
       "  'learning_rate': 4.812000000000001e-05,\n",
       "  'epoch': 1.88,\n",
       "  'step': 188},\n",
       " {'loss': 0.5037,\n",
       "  'learning_rate': 4.8110000000000005e-05,\n",
       "  'epoch': 1.89,\n",
       "  'step': 189},\n",
       " {'loss': 0.4545,\n",
       "  'learning_rate': 4.8100000000000004e-05,\n",
       "  'epoch': 1.9,\n",
       "  'step': 190},\n",
       " {'loss': 0.5303, 'learning_rate': 4.809e-05, 'epoch': 1.91, 'step': 191},\n",
       " {'loss': 0.6896, 'learning_rate': 4.808e-05, 'epoch': 1.92, 'step': 192},\n",
       " {'loss': 0.463,\n",
       "  'learning_rate': 4.8070000000000006e-05,\n",
       "  'epoch': 1.93,\n",
       "  'step': 193},\n",
       " {'loss': 0.5243,\n",
       "  'learning_rate': 4.8060000000000004e-05,\n",
       "  'epoch': 1.94,\n",
       "  'step': 194},\n",
       " {'loss': 0.2599, 'learning_rate': 4.805e-05, 'epoch': 1.95, 'step': 195},\n",
       " {'loss': 0.3991, 'learning_rate': 4.804e-05, 'epoch': 1.96, 'step': 196},\n",
       " {'loss': 0.5667,\n",
       "  'learning_rate': 4.8030000000000006e-05,\n",
       "  'epoch': 1.97,\n",
       "  'step': 197},\n",
       " {'loss': 0.4872,\n",
       "  'learning_rate': 4.8020000000000004e-05,\n",
       "  'epoch': 1.98,\n",
       "  'step': 198},\n",
       " {'loss': 0.3922, 'learning_rate': 4.801e-05, 'epoch': 1.99, 'step': 199},\n",
       " {'loss': 0.4803, 'learning_rate': 4.8e-05, 'epoch': 2.0, 'step': 200},\n",
       " {'eval_loss': 0.7590615153312683,\n",
       "  'eval_accuracy': 0.59,\n",
       "  'eval_runtime': 5.8913,\n",
       "  'eval_samples_per_second': 16.974,\n",
       "  'eval_steps_per_second': 2.207,\n",
       "  'epoch': 2.0,\n",
       "  'step': 200},\n",
       " {'loss': 0.2365, 'learning_rate': 4.799e-05, 'epoch': 2.01, 'step': 201},\n",
       " {'loss': 0.3578,\n",
       "  'learning_rate': 4.7980000000000005e-05,\n",
       "  'epoch': 2.02,\n",
       "  'step': 202},\n",
       " {'loss': 0.4284, 'learning_rate': 4.797e-05, 'epoch': 2.03, 'step': 203},\n",
       " {'loss': 0.2358, 'learning_rate': 4.796e-05, 'epoch': 2.04, 'step': 204},\n",
       " {'loss': 0.1974, 'learning_rate': 4.795e-05, 'epoch': 2.05, 'step': 205},\n",
       " {'loss': 0.3155, 'learning_rate': 4.794e-05, 'epoch': 2.06, 'step': 206},\n",
       " {'loss': 0.448,\n",
       "  'learning_rate': 4.7930000000000004e-05,\n",
       "  'epoch': 2.07,\n",
       "  'step': 207},\n",
       " {'loss': 0.3146, 'learning_rate': 4.792e-05, 'epoch': 2.08, 'step': 208},\n",
       " {'loss': 0.1808,\n",
       "  'learning_rate': 4.791000000000001e-05,\n",
       "  'epoch': 2.09,\n",
       "  'step': 209},\n",
       " {'loss': 0.6028, 'learning_rate': 4.79e-05, 'epoch': 2.1, 'step': 210},\n",
       " {'loss': 0.2514,\n",
       "  'learning_rate': 4.7890000000000004e-05,\n",
       "  'epoch': 2.11,\n",
       "  'step': 211},\n",
       " {'loss': 0.3119, 'learning_rate': 4.788e-05, 'epoch': 2.12, 'step': 212},\n",
       " {'loss': 0.1246, 'learning_rate': 4.787e-05, 'epoch': 2.13, 'step': 213},\n",
       " {'loss': 0.1482,\n",
       "  'learning_rate': 4.7860000000000006e-05,\n",
       "  'epoch': 2.14,\n",
       "  'step': 214},\n",
       " {'loss': 0.1228, 'learning_rate': 4.785e-05, 'epoch': 2.15, 'step': 215},\n",
       " {'loss': 0.1764, 'learning_rate': 4.784e-05, 'epoch': 2.16, 'step': 216},\n",
       " {'loss': 0.1268, 'learning_rate': 4.783e-05, 'epoch': 2.17, 'step': 217},\n",
       " {'loss': 0.0437,\n",
       "  'learning_rate': 4.7820000000000006e-05,\n",
       "  'epoch': 2.18,\n",
       "  'step': 218},\n",
       " {'loss': 0.4457,\n",
       "  'learning_rate': 4.7810000000000005e-05,\n",
       "  'epoch': 2.19,\n",
       "  'step': 219},\n",
       " {'loss': 0.2806, 'learning_rate': 4.78e-05, 'epoch': 2.2, 'step': 220},\n",
       " {'loss': 0.2724, 'learning_rate': 4.779e-05, 'epoch': 2.21, 'step': 221},\n",
       " {'loss': 0.2924, 'learning_rate': 4.778e-05, 'epoch': 2.22, 'step': 222},\n",
       " {'loss': 0.1182,\n",
       "  'learning_rate': 4.7770000000000005e-05,\n",
       "  'epoch': 2.23,\n",
       "  'step': 223},\n",
       " {'loss': 0.1199,\n",
       "  'learning_rate': 4.7760000000000004e-05,\n",
       "  'epoch': 2.24,\n",
       "  'step': 224},\n",
       " {'loss': 0.2145, 'learning_rate': 4.775e-05, 'epoch': 2.25, 'step': 225},\n",
       " {'loss': 0.2087, 'learning_rate': 4.774e-05, 'epoch': 2.26, 'step': 226},\n",
       " {'loss': 0.1256,\n",
       "  'learning_rate': 4.7730000000000005e-05,\n",
       "  'epoch': 2.27,\n",
       "  'step': 227},\n",
       " {'loss': 0.3223,\n",
       "  'learning_rate': 4.7720000000000004e-05,\n",
       "  'epoch': 2.28,\n",
       "  'step': 228},\n",
       " {'loss': 0.1876, 'learning_rate': 4.771e-05, 'epoch': 2.29, 'step': 229},\n",
       " {'loss': 0.1408, 'learning_rate': 4.77e-05, 'epoch': 2.3, 'step': 230},\n",
       " {'loss': 0.0893, 'learning_rate': 4.769e-05, 'epoch': 2.31, 'step': 231},\n",
       " {'loss': 0.41,\n",
       "  'learning_rate': 4.7680000000000004e-05,\n",
       "  'epoch': 2.32,\n",
       "  'step': 232},\n",
       " {'loss': 0.1795, 'learning_rate': 4.767e-05, 'epoch': 2.33, 'step': 233},\n",
       " {'loss': 0.1761,\n",
       "  'learning_rate': 4.766000000000001e-05,\n",
       "  'epoch': 2.34,\n",
       "  'step': 234},\n",
       " {'loss': 0.0155, 'learning_rate': 4.765e-05, 'epoch': 2.35, 'step': 235},\n",
       " {'loss': 0.1209,\n",
       "  'learning_rate': 4.7640000000000005e-05,\n",
       "  'epoch': 2.36,\n",
       "  'step': 236},\n",
       " {'loss': 0.5115, 'learning_rate': 4.763e-05, 'epoch': 2.37, 'step': 237},\n",
       " {'loss': 0.3855, 'learning_rate': 4.762e-05, 'epoch': 2.38, 'step': 238},\n",
       " {'loss': 0.3745,\n",
       "  'learning_rate': 4.761000000000001e-05,\n",
       "  'epoch': 2.39,\n",
       "  'step': 239},\n",
       " {'loss': 0.8172, 'learning_rate': 4.76e-05, 'epoch': 2.4, 'step': 240},\n",
       " {'loss': 0.5196,\n",
       "  'learning_rate': 4.7590000000000003e-05,\n",
       "  'epoch': 2.41,\n",
       "  'step': 241},\n",
       " {'loss': 0.3313, 'learning_rate': 4.758e-05, 'epoch': 2.42, 'step': 242},\n",
       " {'loss': 0.1674, 'learning_rate': 4.757e-05, 'epoch': 2.43, 'step': 243},\n",
       " {'loss': 0.2609,\n",
       "  'learning_rate': 4.7560000000000005e-05,\n",
       "  'epoch': 2.44,\n",
       "  'step': 244},\n",
       " {'loss': 0.6568, 'learning_rate': 4.755e-05, 'epoch': 2.45, 'step': 245},\n",
       " {'loss': 0.6191, 'learning_rate': 4.754e-05, 'epoch': 2.46, 'step': 246},\n",
       " {'loss': 0.1538, 'learning_rate': 4.753e-05, 'epoch': 2.47, 'step': 247},\n",
       " {'loss': 0.3437,\n",
       "  'learning_rate': 4.7520000000000006e-05,\n",
       "  'epoch': 2.48,\n",
       "  'step': 248},\n",
       " {'loss': 0.3312,\n",
       "  'learning_rate': 4.7510000000000004e-05,\n",
       "  'epoch': 2.49,\n",
       "  'step': 249},\n",
       " {'loss': 0.1074, 'learning_rate': 4.75e-05, 'epoch': 2.5, 'step': 250},\n",
       " {'loss': 0.1504, 'learning_rate': 4.749e-05, 'epoch': 2.51, 'step': 251},\n",
       " {'loss': 0.1304, 'learning_rate': 4.748e-05, 'epoch': 2.52, 'step': 252},\n",
       " {'loss': 0.1654,\n",
       "  'learning_rate': 4.7470000000000005e-05,\n",
       "  'epoch': 2.53,\n",
       "  'step': 253},\n",
       " {'loss': 0.7891, 'learning_rate': 4.746e-05, 'epoch': 2.54, 'step': 254},\n",
       " {'loss': 0.15, 'learning_rate': 4.745e-05, 'epoch': 2.55, 'step': 255},\n",
       " {'loss': 0.6939, 'learning_rate': 4.744e-05, 'epoch': 2.56, 'step': 256},\n",
       " {'loss': 0.3359,\n",
       "  'learning_rate': 4.7430000000000005e-05,\n",
       "  'epoch': 2.57,\n",
       "  'step': 257},\n",
       " {'loss': 0.6156, 'learning_rate': 4.742e-05, 'epoch': 2.58, 'step': 258},\n",
       " {'loss': 0.2798, 'learning_rate': 4.741e-05, 'epoch': 2.59, 'step': 259},\n",
       " {'loss': 0.5049, 'learning_rate': 4.74e-05, 'epoch': 2.6, 'step': 260},\n",
       " {'loss': 0.7752, 'learning_rate': 4.739e-05, 'epoch': 2.61, 'step': 261},\n",
       " {'loss': 0.2777,\n",
       "  'learning_rate': 4.7380000000000004e-05,\n",
       "  'epoch': 2.62,\n",
       "  'step': 262},\n",
       " {'loss': 0.3948, 'learning_rate': 4.737e-05, 'epoch': 2.63, 'step': 263},\n",
       " {'loss': 0.2156,\n",
       "  'learning_rate': 4.736000000000001e-05,\n",
       "  'epoch': 2.64,\n",
       "  'step': 264},\n",
       " {'loss': 0.232, 'learning_rate': 4.735e-05, 'epoch': 2.65, 'step': 265},\n",
       " {'loss': 0.4265,\n",
       "  'learning_rate': 4.7340000000000004e-05,\n",
       "  'epoch': 2.66,\n",
       "  'step': 266},\n",
       " {'loss': 0.6989, 'learning_rate': 4.733e-05, 'epoch': 2.67, 'step': 267},\n",
       " {'loss': 0.4284, 'learning_rate': 4.732e-05, 'epoch': 2.68, 'step': 268},\n",
       " {'loss': 0.3048,\n",
       "  'learning_rate': 4.7310000000000006e-05,\n",
       "  'epoch': 2.69,\n",
       "  'step': 269},\n",
       " {'loss': 0.4743, 'learning_rate': 4.73e-05, 'epoch': 2.7, 'step': 270},\n",
       " {'loss': 0.1587, 'learning_rate': 4.729e-05, 'epoch': 2.71, 'step': 271},\n",
       " {'loss': 0.2714, 'learning_rate': 4.728e-05, 'epoch': 2.72, 'step': 272},\n",
       " {'loss': 0.1098,\n",
       "  'learning_rate': 4.7270000000000007e-05,\n",
       "  'epoch': 2.73,\n",
       "  'step': 273},\n",
       " {'loss': 0.2706,\n",
       "  'learning_rate': 4.7260000000000005e-05,\n",
       "  'epoch': 2.74,\n",
       "  'step': 274},\n",
       " {'loss': 0.8212,\n",
       "  'learning_rate': 4.7249999999999997e-05,\n",
       "  'epoch': 2.75,\n",
       "  'step': 275},\n",
       " {'loss': 0.1627, 'learning_rate': 4.724e-05, 'epoch': 2.76, 'step': 276},\n",
       " {'loss': 0.3697, 'learning_rate': 4.723e-05, 'epoch': 2.77, 'step': 277},\n",
       " {'loss': 0.2888,\n",
       "  'learning_rate': 4.7220000000000005e-05,\n",
       "  'epoch': 2.78,\n",
       "  'step': 278},\n",
       " {'loss': 0.2143,\n",
       "  'learning_rate': 4.7210000000000004e-05,\n",
       "  'epoch': 2.79,\n",
       "  'step': 279},\n",
       " {'loss': 0.4301, 'learning_rate': 4.72e-05, 'epoch': 2.8, 'step': 280},\n",
       " {'loss': 0.2535, 'learning_rate': 4.719e-05, 'epoch': 2.81, 'step': 281},\n",
       " {'loss': 0.4388, 'learning_rate': 4.718e-05, 'epoch': 2.82, 'step': 282},\n",
       " {'loss': 0.7164,\n",
       "  'learning_rate': 4.7170000000000004e-05,\n",
       "  'epoch': 2.83,\n",
       "  'step': 283},\n",
       " {'loss': 0.0782, 'learning_rate': 4.716e-05, 'epoch': 2.84, 'step': 284},\n",
       " {'loss': 0.3112, 'learning_rate': 4.715e-05, 'epoch': 2.85, 'step': 285},\n",
       " {'loss': 0.039, 'learning_rate': 4.714e-05, 'epoch': 2.86, 'step': 286},\n",
       " {'loss': 0.3948,\n",
       "  'learning_rate': 4.7130000000000004e-05,\n",
       "  'epoch': 2.87,\n",
       "  'step': 287},\n",
       " {'loss': 0.1151, 'learning_rate': 4.712e-05, 'epoch': 2.88, 'step': 288},\n",
       " {'loss': 0.2533, 'learning_rate': 4.711e-05, 'epoch': 2.89, 'step': 289},\n",
       " {'loss': 0.0635, 'learning_rate': 4.71e-05, 'epoch': 2.9, 'step': 290},\n",
       " {'loss': 0.0677, 'learning_rate': 4.709e-05, 'epoch': 2.91, 'step': 291},\n",
       " {'loss': 0.2747, 'learning_rate': 4.708e-05, 'epoch': 2.92, 'step': 292},\n",
       " {'loss': 0.0935, 'learning_rate': 4.707e-05, 'epoch': 2.93, 'step': 293},\n",
       " {'loss': 0.2762,\n",
       "  'learning_rate': 4.706000000000001e-05,\n",
       "  'epoch': 2.94,\n",
       "  'step': 294},\n",
       " {'loss': 0.5046, 'learning_rate': 4.705e-05, 'epoch': 2.95, 'step': 295},\n",
       " {'loss': 0.1889,\n",
       "  'learning_rate': 4.7040000000000004e-05,\n",
       "  'epoch': 2.96,\n",
       "  'step': 296},\n",
       " {'loss': 0.2072, 'learning_rate': 4.703e-05, 'epoch': 2.97, 'step': 297},\n",
       " {'loss': 0.0929, 'learning_rate': 4.702e-05, 'epoch': 2.98, 'step': 298},\n",
       " {'loss': 0.0537,\n",
       "  'learning_rate': 4.7010000000000006e-05,\n",
       "  'epoch': 2.99,\n",
       "  'step': 299},\n",
       " {'loss': 0.2164, 'learning_rate': 4.7e-05, 'epoch': 3.0, 'step': 300},\n",
       " {'eval_loss': 1.1043028831481934,\n",
       "  'eval_accuracy': 0.55,\n",
       "  'eval_runtime': 5.8853,\n",
       "  'eval_samples_per_second': 16.991,\n",
       "  'eval_steps_per_second': 2.209,\n",
       "  'epoch': 3.0,\n",
       "  'step': 300},\n",
       " {'loss': 0.0584, 'learning_rate': 4.699e-05, 'epoch': 3.01, 'step': 301},\n",
       " {'loss': 0.0479, 'learning_rate': 4.698e-05, 'epoch': 3.02, 'step': 302},\n",
       " {'loss': 0.1578,\n",
       "  'learning_rate': 4.6970000000000006e-05,\n",
       "  'epoch': 3.03,\n",
       "  'step': 303},\n",
       " {'loss': 0.0084,\n",
       "  'learning_rate': 4.6960000000000004e-05,\n",
       "  'epoch': 3.04,\n",
       "  'step': 304},\n",
       " {'loss': 0.0764, 'learning_rate': 4.695e-05, 'epoch': 3.05, 'step': 305},\n",
       " {'loss': 0.1468, 'learning_rate': 4.694e-05, 'epoch': 3.06, 'step': 306},\n",
       " {'loss': 0.0143, 'learning_rate': 4.693e-05, 'epoch': 3.07, 'step': 307},\n",
       " {'loss': 0.0148,\n",
       "  'learning_rate': 4.6920000000000005e-05,\n",
       "  'epoch': 3.08,\n",
       "  'step': 308},\n",
       " {'loss': 0.1016, 'learning_rate': 4.691e-05, 'epoch': 3.09, 'step': 309},\n",
       " {'loss': 0.1152, 'learning_rate': 4.69e-05, 'epoch': 3.1, 'step': 310},\n",
       " {'loss': 0.0549, 'learning_rate': 4.689e-05, 'epoch': 3.11, 'step': 311},\n",
       " {'loss': 0.0488, 'learning_rate': 4.688e-05, 'epoch': 3.12, 'step': 312},\n",
       " {'loss': 0.0169,\n",
       "  'learning_rate': 4.6870000000000004e-05,\n",
       "  'epoch': 3.13,\n",
       "  'step': 313},\n",
       " {'loss': 0.0757, 'learning_rate': 4.686e-05, 'epoch': 3.14, 'step': 314},\n",
       " {'loss': 0.0133,\n",
       "  'learning_rate': 4.685000000000001e-05,\n",
       "  'epoch': 3.15,\n",
       "  'step': 315},\n",
       " {'loss': 0.0058, 'learning_rate': 4.684e-05, 'epoch': 3.16, 'step': 316},\n",
       " {'loss': 0.0021,\n",
       "  'learning_rate': 4.6830000000000004e-05,\n",
       "  'epoch': 3.17,\n",
       "  'step': 317},\n",
       " {'loss': 0.0359, 'learning_rate': 4.682e-05, 'epoch': 3.18, 'step': 318},\n",
       " {'loss': 0.071, 'learning_rate': 4.681e-05, 'epoch': 3.19, 'step': 319},\n",
       " {'loss': 0.0394,\n",
       "  'learning_rate': 4.6800000000000006e-05,\n",
       "  'epoch': 3.2,\n",
       "  'step': 320},\n",
       " {'loss': 0.0314, 'learning_rate': 4.679e-05, 'epoch': 3.21, 'step': 321},\n",
       " {'loss': 0.097, 'learning_rate': 4.678e-05, 'epoch': 3.22, 'step': 322},\n",
       " {'loss': 0.0769, 'learning_rate': 4.677e-05, 'epoch': 3.23, 'step': 323},\n",
       " {'loss': 0.7726,\n",
       "  'learning_rate': 4.6760000000000006e-05,\n",
       "  'epoch': 3.24,\n",
       "  'step': 324},\n",
       " {'loss': 0.0533,\n",
       "  'learning_rate': 4.6750000000000005e-05,\n",
       "  'epoch': 3.25,\n",
       "  'step': 325},\n",
       " {'loss': 0.0311, 'learning_rate': 4.674e-05, 'epoch': 3.26, 'step': 326},\n",
       " {'loss': 0.1278, 'learning_rate': 4.673e-05, 'epoch': 3.27, 'step': 327},\n",
       " {'loss': 0.1049, 'learning_rate': 4.672e-05, 'epoch': 3.28, 'step': 328},\n",
       " {'loss': 0.0275,\n",
       "  'learning_rate': 4.6710000000000005e-05,\n",
       "  'epoch': 3.29,\n",
       "  'step': 329},\n",
       " {'loss': 0.1159,\n",
       "  'learning_rate': 4.6700000000000003e-05,\n",
       "  'epoch': 3.3,\n",
       "  'step': 330},\n",
       " {'loss': 0.11, 'learning_rate': 4.669e-05, 'epoch': 3.31, 'step': 331},\n",
       " {'loss': 0.1026, 'learning_rate': 4.668e-05, 'epoch': 3.32, 'step': 332},\n",
       " {'loss': 0.0946,\n",
       "  'learning_rate': 4.6670000000000005e-05,\n",
       "  'epoch': 3.33,\n",
       "  'step': 333},\n",
       " {'loss': 1.0378,\n",
       "  'learning_rate': 4.6660000000000004e-05,\n",
       "  'epoch': 3.34,\n",
       "  'step': 334},\n",
       " {'loss': 0.1776, 'learning_rate': 4.665e-05, 'epoch': 3.35, 'step': 335},\n",
       " {'loss': 0.0103, 'learning_rate': 4.664e-05, 'epoch': 3.36, 'step': 336},\n",
       " {'loss': 0.1093, 'learning_rate': 4.663e-05, 'epoch': 3.37, 'step': 337},\n",
       " {'loss': 0.0747,\n",
       "  'learning_rate': 4.6620000000000004e-05,\n",
       "  'epoch': 3.38,\n",
       "  'step': 338},\n",
       " {'loss': 0.0486, 'learning_rate': 4.661e-05, 'epoch': 3.39, 'step': 339},\n",
       " {'loss': 0.0113,\n",
       "  'learning_rate': 4.660000000000001e-05,\n",
       "  'epoch': 3.4,\n",
       "  'step': 340},\n",
       " {'loss': 0.5404, 'learning_rate': 4.659e-05, 'epoch': 3.41, 'step': 341},\n",
       " {'loss': 0.1005,\n",
       "  'learning_rate': 4.6580000000000005e-05,\n",
       "  'epoch': 3.42,\n",
       "  'step': 342},\n",
       " {'loss': 0.0497, 'learning_rate': 4.657e-05, 'epoch': 3.43, 'step': 343},\n",
       " {'loss': 0.0671, 'learning_rate': 4.656e-05, 'epoch': 3.44, 'step': 344},\n",
       " {'loss': 0.033,\n",
       "  'learning_rate': 4.655000000000001e-05,\n",
       "  'epoch': 3.45,\n",
       "  'step': 345},\n",
       " {'loss': 0.0615, 'learning_rate': 4.654e-05, 'epoch': 3.46, 'step': 346},\n",
       " {'loss': 0.0524,\n",
       "  'learning_rate': 4.6530000000000003e-05,\n",
       "  'epoch': 3.47,\n",
       "  'step': 347},\n",
       " {'loss': 0.1956, 'learning_rate': 4.652e-05, 'epoch': 3.48, 'step': 348},\n",
       " {'loss': 0.0323, 'learning_rate': 4.651e-05, 'epoch': 3.49, 'step': 349},\n",
       " {'loss': 1.0593,\n",
       "  'learning_rate': 4.6500000000000005e-05,\n",
       "  'epoch': 3.5,\n",
       "  'step': 350},\n",
       " {'loss': 0.3288, 'learning_rate': 4.649e-05, 'epoch': 3.51, 'step': 351},\n",
       " {'loss': 0.2196, 'learning_rate': 4.648e-05, 'epoch': 3.52, 'step': 352},\n",
       " {'loss': 0.2422, 'learning_rate': 4.647e-05, 'epoch': 3.53, 'step': 353},\n",
       " {'loss': 0.0819,\n",
       "  'learning_rate': 4.6460000000000006e-05,\n",
       "  'epoch': 3.54,\n",
       "  'step': 354},\n",
       " {'loss': 0.0707,\n",
       "  'learning_rate': 4.6450000000000004e-05,\n",
       "  'epoch': 3.55,\n",
       "  'step': 355},\n",
       " {'loss': 0.2662, 'learning_rate': 4.644e-05, 'epoch': 3.56, 'step': 356},\n",
       " {'loss': 0.0228, 'learning_rate': 4.643e-05, 'epoch': 3.57, 'step': 357},\n",
       " {'loss': 0.1192, 'learning_rate': 4.642e-05, 'epoch': 3.58, 'step': 358},\n",
       " {'loss': 0.1638,\n",
       "  'learning_rate': 4.6410000000000005e-05,\n",
       "  'epoch': 3.59,\n",
       "  'step': 359},\n",
       " {'loss': 0.025, 'learning_rate': 4.64e-05, 'epoch': 3.6, 'step': 360},\n",
       " {'loss': 0.0542, 'learning_rate': 4.639e-05, 'epoch': 3.61, 'step': 361},\n",
       " {'loss': 0.0402, 'learning_rate': 4.638e-05, 'epoch': 3.62, 'step': 362},\n",
       " {'loss': 0.0429,\n",
       "  'learning_rate': 4.6370000000000005e-05,\n",
       "  'epoch': 3.63,\n",
       "  'step': 363},\n",
       " {'loss': 0.1665, 'learning_rate': 4.636e-05, 'epoch': 3.64, 'step': 364},\n",
       " {'loss': 0.2253, 'learning_rate': 4.635e-05, 'epoch': 3.65, 'step': 365},\n",
       " {'loss': 0.6763, 'learning_rate': 4.634e-05, 'epoch': 3.66, 'step': 366},\n",
       " {'loss': 0.0286, 'learning_rate': 4.633e-05, 'epoch': 3.67, 'step': 367},\n",
       " {'loss': 0.0931,\n",
       "  'learning_rate': 4.6320000000000004e-05,\n",
       "  'epoch': 3.68,\n",
       "  'step': 368},\n",
       " {'loss': 0.0665, 'learning_rate': 4.631e-05, 'epoch': 3.69, 'step': 369},\n",
       " {'loss': 0.6285,\n",
       "  'learning_rate': 4.630000000000001e-05,\n",
       "  'epoch': 3.7,\n",
       "  'step': 370},\n",
       " {'loss': 0.0153, 'learning_rate': 4.629e-05, 'epoch': 3.71, 'step': 371},\n",
       " {'loss': 0.0344,\n",
       "  'learning_rate': 4.6280000000000004e-05,\n",
       "  'epoch': 3.72,\n",
       "  'step': 372},\n",
       " {'loss': 0.0188, 'learning_rate': 4.627e-05, 'epoch': 3.73, 'step': 373},\n",
       " {'loss': 0.2043, 'learning_rate': 4.626e-05, 'epoch': 3.74, 'step': 374},\n",
       " {'loss': 0.0802,\n",
       "  'learning_rate': 4.6250000000000006e-05,\n",
       "  'epoch': 3.75,\n",
       "  'step': 375},\n",
       " {'loss': 0.4602, 'learning_rate': 4.624e-05, 'epoch': 3.76, 'step': 376},\n",
       " {'loss': 0.4592, 'learning_rate': 4.623e-05, 'epoch': 3.77, 'step': 377},\n",
       " {'loss': 0.0118, 'learning_rate': 4.622e-05, 'epoch': 3.78, 'step': 378},\n",
       " {'loss': 0.178,\n",
       "  'learning_rate': 4.6210000000000006e-05,\n",
       "  'epoch': 3.79,\n",
       "  'step': 379},\n",
       " {'loss': 0.0294,\n",
       "  'learning_rate': 4.6200000000000005e-05,\n",
       "  'epoch': 3.8,\n",
       "  'step': 380},\n",
       " {'loss': 0.0613, 'learning_rate': 4.619e-05, 'epoch': 3.81, 'step': 381},\n",
       " {'loss': 0.0014, 'learning_rate': 4.618e-05, 'epoch': 3.82, 'step': 382},\n",
       " {'loss': 0.0718, 'learning_rate': 4.617e-05, 'epoch': 3.83, 'step': 383},\n",
       " {'loss': 0.0068,\n",
       "  'learning_rate': 4.6160000000000005e-05,\n",
       "  'epoch': 3.84,\n",
       "  'step': 384},\n",
       " {'loss': 0.0755,\n",
       "  'learning_rate': 4.6150000000000004e-05,\n",
       "  'epoch': 3.85,\n",
       "  'step': 385},\n",
       " {'loss': 0.1838, 'learning_rate': 4.614e-05, 'epoch': 3.86, 'step': 386},\n",
       " {'loss': 0.8791, 'learning_rate': 4.613e-05, 'epoch': 3.87, 'step': 387},\n",
       " {'loss': 0.1253, 'learning_rate': 4.612e-05, 'epoch': 3.88, 'step': 388},\n",
       " {'loss': 0.0078,\n",
       "  'learning_rate': 4.6110000000000004e-05,\n",
       "  'epoch': 3.89,\n",
       "  'step': 389},\n",
       " {'loss': 0.0155, 'learning_rate': 4.61e-05, 'epoch': 3.9, 'step': 390},\n",
       " {'loss': 0.3081, 'learning_rate': 4.609e-05, 'epoch': 3.91, 'step': 391},\n",
       " {'loss': 0.3174, 'learning_rate': 4.608e-05, 'epoch': 3.92, 'step': 392},\n",
       " {'loss': 0.0449,\n",
       "  'learning_rate': 4.6070000000000004e-05,\n",
       "  'epoch': 3.93,\n",
       "  'step': 393},\n",
       " {'loss': 0.0479, 'learning_rate': 4.606e-05, 'epoch': 3.94, 'step': 394},\n",
       " {'loss': 0.0528, 'learning_rate': 4.605e-05, 'epoch': 3.95, 'step': 395},\n",
       " {'loss': 0.3398, 'learning_rate': 4.604e-05, 'epoch': 3.96, 'step': 396},\n",
       " {'loss': 0.3106, 'learning_rate': 4.603e-05, 'epoch': 3.97, 'step': 397},\n",
       " {'loss': 0.0846, 'learning_rate': 4.602e-05, 'epoch': 3.98, 'step': 398},\n",
       " {'loss': 0.0675, 'learning_rate': 4.601e-05, 'epoch': 3.99, 'step': 399},\n",
       " {'loss': 0.0471,\n",
       "  'learning_rate': 4.600000000000001e-05,\n",
       "  'epoch': 4.0,\n",
       "  'step': 400},\n",
       " {'eval_loss': 1.2031238079071045,\n",
       "  'eval_accuracy': 0.56,\n",
       "  'eval_runtime': 5.8824,\n",
       "  'eval_samples_per_second': 17.0,\n",
       "  'eval_steps_per_second': 2.21,\n",
       "  'epoch': 4.0,\n",
       "  'step': 400},\n",
       " {'loss': 0.0133, 'learning_rate': 4.599e-05, 'epoch': 4.01, 'step': 401},\n",
       " {'loss': 0.029,\n",
       "  'learning_rate': 4.5980000000000004e-05,\n",
       "  'epoch': 4.02,\n",
       "  'step': 402},\n",
       " {'loss': 0.0051, 'learning_rate': 4.597e-05, 'epoch': 4.03, 'step': 403},\n",
       " {'loss': 0.0386, 'learning_rate': 4.596e-05, 'epoch': 4.04, 'step': 404},\n",
       " {'loss': 0.0112,\n",
       "  'learning_rate': 4.5950000000000006e-05,\n",
       "  'epoch': 4.05,\n",
       "  'step': 405},\n",
       " {'loss': 0.3507, 'learning_rate': 4.594e-05, 'epoch': 4.06, 'step': 406},\n",
       " {'loss': 0.0341, 'learning_rate': 4.593e-05, 'epoch': 4.07, 'step': 407},\n",
       " {'loss': 0.3167, 'learning_rate': 4.592e-05, 'epoch': 4.08, 'step': 408},\n",
       " {'loss': 0.0664,\n",
       "  'learning_rate': 4.5910000000000006e-05,\n",
       "  'epoch': 4.09,\n",
       "  'step': 409},\n",
       " {'loss': 0.0072,\n",
       "  'learning_rate': 4.5900000000000004e-05,\n",
       "  'epoch': 4.1,\n",
       "  'step': 410},\n",
       " {'loss': 0.0261, 'learning_rate': 4.589e-05, 'epoch': 4.11, 'step': 411},\n",
       " {'loss': 0.0088, 'learning_rate': 4.588e-05, 'epoch': 4.12, 'step': 412},\n",
       " {'loss': 0.002, 'learning_rate': 4.587e-05, 'epoch': 4.13, 'step': 413},\n",
       " {'loss': 0.1689,\n",
       "  'learning_rate': 4.5860000000000005e-05,\n",
       "  'epoch': 4.14,\n",
       "  'step': 414},\n",
       " {'loss': 0.158, 'learning_rate': 4.585e-05, 'epoch': 4.15, 'step': 415},\n",
       " {'loss': 0.148, 'learning_rate': 4.584e-05, 'epoch': 4.16, 'step': 416},\n",
       " {'loss': 0.3353, 'learning_rate': 4.583e-05, 'epoch': 4.17, 'step': 417},\n",
       " {'loss': 0.2026,\n",
       "  'learning_rate': 4.5820000000000005e-05,\n",
       "  'epoch': 4.18,\n",
       "  'step': 418},\n",
       " {'loss': 0.2993,\n",
       "  'learning_rate': 4.5810000000000004e-05,\n",
       "  'epoch': 4.19,\n",
       "  'step': 419},\n",
       " {'loss': 0.1118, 'learning_rate': 4.58e-05, 'epoch': 4.2, 'step': 420},\n",
       " {'loss': 0.1646, 'learning_rate': 4.579e-05, 'epoch': 4.21, 'step': 421},\n",
       " {'loss': 0.138, 'learning_rate': 4.578e-05, 'epoch': 4.22, 'step': 422},\n",
       " {'loss': 0.0486,\n",
       "  'learning_rate': 4.5770000000000004e-05,\n",
       "  'epoch': 4.23,\n",
       "  'step': 423},\n",
       " {'loss': 0.1005, 'learning_rate': 4.576e-05, 'epoch': 4.24, 'step': 424},\n",
       " {'loss': 0.2125, 'learning_rate': 4.575e-05, 'epoch': 4.25, 'step': 425},\n",
       " {'loss': 0.0104, 'learning_rate': 4.574e-05, 'epoch': 4.26, 'step': 426},\n",
       " {'loss': 0.0083, 'learning_rate': 4.573e-05, 'epoch': 4.27, 'step': 427},\n",
       " {'loss': 0.0851, 'learning_rate': 4.572e-05, 'epoch': 4.28, 'step': 428},\n",
       " {'loss': 0.006, 'learning_rate': 4.571e-05, 'epoch': 4.29, 'step': 429},\n",
       " {'loss': 0.0717,\n",
       "  'learning_rate': 4.5700000000000006e-05,\n",
       "  'epoch': 4.3,\n",
       "  'step': 430},\n",
       " {'loss': 0.0027, 'learning_rate': 4.569e-05, 'epoch': 4.31, 'step': 431},\n",
       " {'loss': 0.3827, 'learning_rate': 4.568e-05, 'epoch': 4.32, 'step': 432},\n",
       " {'loss': 0.0246, 'learning_rate': 4.567e-05, 'epoch': 4.33, 'step': 433},\n",
       " {'loss': 0.0005, 'learning_rate': 4.566e-05, 'epoch': 4.34, 'step': 434},\n",
       " {'loss': 0.4475,\n",
       "  'learning_rate': 4.5650000000000005e-05,\n",
       "  'epoch': 4.35,\n",
       "  'step': 435},\n",
       " {'loss': 0.1843, 'learning_rate': 4.564e-05, 'epoch': 4.36, 'step': 436},\n",
       " {'loss': 0.0195, 'learning_rate': 4.563e-05, 'epoch': 4.37, 'step': 437},\n",
       " {'loss': 0.0177, 'learning_rate': 4.562e-05, 'epoch': 4.38, 'step': 438},\n",
       " {'loss': 0.1386,\n",
       "  'learning_rate': 4.5610000000000005e-05,\n",
       "  'epoch': 4.39,\n",
       "  'step': 439},\n",
       " {'loss': 0.1032,\n",
       "  'learning_rate': 4.5600000000000004e-05,\n",
       "  'epoch': 4.4,\n",
       "  'step': 440},\n",
       " {'loss': 0.224, 'learning_rate': 4.559e-05, 'epoch': 4.41, 'step': 441},\n",
       " {'loss': 0.0688, 'learning_rate': 4.558e-05, 'epoch': 4.42, 'step': 442},\n",
       " {'loss': 0.0913, 'learning_rate': 4.557e-05, 'epoch': 4.43, 'step': 443},\n",
       " {'loss': 0.0573,\n",
       "  'learning_rate': 4.5560000000000004e-05,\n",
       "  'epoch': 4.44,\n",
       "  'step': 444},\n",
       " {'loss': 0.0911, 'learning_rate': 4.555e-05, 'epoch': 4.45, 'step': 445},\n",
       " {'loss': 0.0562,\n",
       "  'learning_rate': 4.554000000000001e-05,\n",
       "  'epoch': 4.46,\n",
       "  'step': 446},\n",
       " {'loss': 0.1243, 'learning_rate': 4.553e-05, 'epoch': 4.47, 'step': 447},\n",
       " {'loss': 0.228,\n",
       "  'learning_rate': 4.5520000000000005e-05,\n",
       "  'epoch': 4.48,\n",
       "  'step': 448},\n",
       " {'loss': 0.1784, 'learning_rate': 4.551e-05, 'epoch': 4.49, 'step': 449},\n",
       " {'loss': 0.0227, 'learning_rate': 4.55e-05, 'epoch': 4.5, 'step': 450},\n",
       " {'loss': 0.1402,\n",
       "  'learning_rate': 4.549000000000001e-05,\n",
       "  'epoch': 4.51,\n",
       "  'step': 451},\n",
       " {'loss': 0.1367, 'learning_rate': 4.548e-05, 'epoch': 4.52, 'step': 452},\n",
       " {'loss': 0.0007,\n",
       "  'learning_rate': 4.5470000000000003e-05,\n",
       "  'epoch': 4.53,\n",
       "  'step': 453},\n",
       " {'loss': 0.0556, 'learning_rate': 4.546e-05, 'epoch': 4.54, 'step': 454},\n",
       " {'loss': 0.0601,\n",
       "  'learning_rate': 4.545000000000001e-05,\n",
       "  'epoch': 4.55,\n",
       "  'step': 455},\n",
       " {'loss': 0.0078,\n",
       "  'learning_rate': 4.5440000000000005e-05,\n",
       "  'epoch': 4.56,\n",
       "  'step': 456},\n",
       " {'loss': 0.0181, 'learning_rate': 4.543e-05, 'epoch': 4.57, 'step': 457},\n",
       " {'loss': 0.135, 'learning_rate': 4.542e-05, 'epoch': 4.58, 'step': 458},\n",
       " {'loss': 0.0793, 'learning_rate': 4.541e-05, 'epoch': 4.59, 'step': 459},\n",
       " {'loss': 0.0592,\n",
       "  'learning_rate': 4.5400000000000006e-05,\n",
       "  'epoch': 4.6,\n",
       "  'step': 460},\n",
       " {'loss': 0.0026,\n",
       "  'learning_rate': 4.5390000000000004e-05,\n",
       "  'epoch': 4.61,\n",
       "  'step': 461},\n",
       " {'loss': 0.0015, 'learning_rate': 4.538e-05, 'epoch': 4.62, 'step': 462},\n",
       " {'loss': 0.0569, 'learning_rate': 4.537e-05, 'epoch': 4.63, 'step': 463},\n",
       " {'loss': 0.0222, 'learning_rate': 4.536e-05, 'epoch': 4.64, 'step': 464},\n",
       " {'loss': 0.2376,\n",
       "  'learning_rate': 4.5350000000000005e-05,\n",
       "  'epoch': 4.65,\n",
       "  'step': 465},\n",
       " {'loss': 0.0009, 'learning_rate': 4.534e-05, 'epoch': 4.66, 'step': 466},\n",
       " {'loss': 0.2783, 'learning_rate': 4.533e-05, 'epoch': 4.67, 'step': 467},\n",
       " {'loss': 1.127, 'learning_rate': 4.532e-05, 'epoch': 4.68, 'step': 468},\n",
       " {'loss': 0.0147,\n",
       "  'learning_rate': 4.5310000000000005e-05,\n",
       "  'epoch': 4.69,\n",
       "  'step': 469},\n",
       " {'loss': 0.0468, 'learning_rate': 4.53e-05, 'epoch': 4.7, 'step': 470},\n",
       " {'loss': 0.007, 'learning_rate': 4.529e-05, 'epoch': 4.71, 'step': 471},\n",
       " {'loss': 0.0067, 'learning_rate': 4.528e-05, 'epoch': 4.72, 'step': 472},\n",
       " {'loss': 0.068, 'learning_rate': 4.527e-05, 'epoch': 4.73, 'step': 473},\n",
       " {'loss': 0.0024,\n",
       "  'learning_rate': 4.5260000000000004e-05,\n",
       "  'epoch': 4.74,\n",
       "  'step': 474},\n",
       " {'loss': 0.1416, 'learning_rate': 4.525e-05, 'epoch': 4.75, 'step': 475},\n",
       " {'loss': 0.075,\n",
       "  'learning_rate': 4.524000000000001e-05,\n",
       "  'epoch': 4.76,\n",
       "  'step': 476},\n",
       " {'loss': 0.0418, 'learning_rate': 4.523e-05, 'epoch': 4.77, 'step': 477},\n",
       " {'loss': 0.0321,\n",
       "  'learning_rate': 4.5220000000000004e-05,\n",
       "  'epoch': 4.78,\n",
       "  'step': 478},\n",
       " {'loss': 0.0603, 'learning_rate': 4.521e-05, 'epoch': 4.79, 'step': 479},\n",
       " {'loss': 0.0177, 'learning_rate': 4.52e-05, 'epoch': 4.8, 'step': 480},\n",
       " {'loss': 0.0019,\n",
       "  'learning_rate': 4.5190000000000006e-05,\n",
       "  'epoch': 4.81,\n",
       "  'step': 481},\n",
       " {'loss': 0.0073, 'learning_rate': 4.518e-05, 'epoch': 4.82, 'step': 482},\n",
       " {'loss': 0.0211, 'learning_rate': 4.517e-05, 'epoch': 4.83, 'step': 483},\n",
       " {'loss': 0.0044, 'learning_rate': 4.516e-05, 'epoch': 4.84, 'step': 484},\n",
       " {'loss': 0.0031,\n",
       "  'learning_rate': 4.5150000000000006e-05,\n",
       "  'epoch': 4.85,\n",
       "  'step': 485},\n",
       " {'loss': 0.0007,\n",
       "  'learning_rate': 4.5140000000000005e-05,\n",
       "  'epoch': 4.86,\n",
       "  'step': 486},\n",
       " {'loss': 0.0066, 'learning_rate': 4.513e-05, 'epoch': 4.87, 'step': 487},\n",
       " {'loss': 0.073, 'learning_rate': 4.512e-05, 'epoch': 4.88, 'step': 488},\n",
       " {'loss': 0.0021, 'learning_rate': 4.511e-05, 'epoch': 4.89, 'step': 489},\n",
       " {'loss': 0.0587,\n",
       "  'learning_rate': 4.5100000000000005e-05,\n",
       "  'epoch': 4.9,\n",
       "  'step': 490},\n",
       " {'loss': 1.3827,\n",
       "  'learning_rate': 4.5090000000000004e-05,\n",
       "  'epoch': 4.91,\n",
       "  'step': 491},\n",
       " {'loss': 0.0028, 'learning_rate': 4.508e-05, 'epoch': 4.92, 'step': 492},\n",
       " {'loss': 0.691, 'learning_rate': 4.507e-05, 'epoch': 4.93, 'step': 493},\n",
       " {'loss': 1.4242, 'learning_rate': 4.506e-05, 'epoch': 4.94, 'step': 494},\n",
       " {'loss': 0.0015,\n",
       "  'learning_rate': 4.5050000000000004e-05,\n",
       "  'epoch': 4.95,\n",
       "  'step': 495},\n",
       " {'loss': 0.2454, 'learning_rate': 4.504e-05, 'epoch': 4.96, 'step': 496},\n",
       " {'loss': 0.006, 'learning_rate': 4.503e-05, 'epoch': 4.97, 'step': 497},\n",
       " {'loss': 0.0356, 'learning_rate': 4.502e-05, 'epoch': 4.98, 'step': 498},\n",
       " {'loss': 0.0125,\n",
       "  'learning_rate': 4.5010000000000004e-05,\n",
       "  'epoch': 4.99,\n",
       "  'step': 499},\n",
       " {'loss': 0.0342, 'learning_rate': 4.5e-05, 'epoch': 5.0, 'step': 500},\n",
       " {'eval_loss': 1.325342059135437,\n",
       "  'eval_accuracy': 0.59,\n",
       "  'eval_runtime': 5.8969,\n",
       "  'eval_samples_per_second': 16.958,\n",
       "  'eval_steps_per_second': 2.205,\n",
       "  'epoch': 5.0,\n",
       "  'step': 500},\n",
       " {'loss': 0.0091, 'learning_rate': 4.499e-05, 'epoch': 5.01, 'step': 501},\n",
       " {'loss': 1.0003, 'learning_rate': 4.498e-05, 'epoch': 5.02, 'step': 502},\n",
       " {'loss': 0.0023, 'learning_rate': 4.497e-05, 'epoch': 5.03, 'step': 503},\n",
       " {'loss': 1.1464, 'learning_rate': 4.496e-05, 'epoch': 5.04, 'step': 504},\n",
       " {'loss': 0.0069, 'learning_rate': 4.495e-05, 'epoch': 5.05, 'step': 505},\n",
       " {'loss': 0.0037,\n",
       "  'learning_rate': 4.494000000000001e-05,\n",
       "  'epoch': 5.06,\n",
       "  'step': 506},\n",
       " {'loss': 0.0359, 'learning_rate': 4.493e-05, 'epoch': 5.07, 'step': 507},\n",
       " {'loss': 0.0253,\n",
       "  'learning_rate': 4.4920000000000004e-05,\n",
       "  'epoch': 5.08,\n",
       "  'step': 508},\n",
       " {'loss': 0.0893, 'learning_rate': 4.491e-05, 'epoch': 5.09, 'step': 509},\n",
       " {'loss': 0.0011, 'learning_rate': 4.49e-05, 'epoch': 5.1, 'step': 510},\n",
       " {'loss': 0.0048,\n",
       "  'learning_rate': 4.4890000000000006e-05,\n",
       "  'epoch': 5.11,\n",
       "  'step': 511},\n",
       " {'loss': 0.0011, 'learning_rate': 4.488e-05, 'epoch': 5.12, 'step': 512},\n",
       " {'loss': 0.1093, 'learning_rate': 4.487e-05, 'epoch': 5.13, 'step': 513},\n",
       " {'loss': 0.0016, 'learning_rate': 4.486e-05, 'epoch': 5.14, 'step': 514},\n",
       " {'loss': 0.244,\n",
       "  'learning_rate': 4.4850000000000006e-05,\n",
       "  'epoch': 5.15,\n",
       "  'step': 515},\n",
       " {'loss': 0.0006,\n",
       "  'learning_rate': 4.4840000000000004e-05,\n",
       "  'epoch': 5.16,\n",
       "  'step': 516},\n",
       " {'loss': 0.1441, 'learning_rate': 4.483e-05, 'epoch': 5.17, 'step': 517},\n",
       " {'loss': 0.0002, 'learning_rate': 4.482e-05, 'epoch': 5.18, 'step': 518},\n",
       " {'loss': 0.0206, 'learning_rate': 4.481e-05, 'epoch': 5.19, 'step': 519},\n",
       " {'loss': 0.0147,\n",
       "  'learning_rate': 4.4800000000000005e-05,\n",
       "  'epoch': 5.2,\n",
       "  'step': 520},\n",
       " {'loss': 0.0019, 'learning_rate': 4.479e-05, 'epoch': 5.21, 'step': 521},\n",
       " {'loss': 0.0011, 'learning_rate': 4.478e-05, 'epoch': 5.22, 'step': 522},\n",
       " {'loss': 0.0023, 'learning_rate': 4.477e-05, 'epoch': 5.23, 'step': 523},\n",
       " {'loss': 0.0003,\n",
       "  'learning_rate': 4.4760000000000005e-05,\n",
       "  'epoch': 5.24,\n",
       "  'step': 524},\n",
       " {'loss': 0.0021,\n",
       "  'learning_rate': 4.4750000000000004e-05,\n",
       "  'epoch': 5.25,\n",
       "  'step': 525},\n",
       " {'loss': 0.0264, 'learning_rate': 4.474e-05, 'epoch': 5.26, 'step': 526},\n",
       " {'loss': 0.0019, 'learning_rate': 4.473e-05, 'epoch': 5.27, 'step': 527},\n",
       " {'loss': 0.0597, 'learning_rate': 4.472e-05, 'epoch': 5.28, 'step': 528},\n",
       " {'loss': 0.0012,\n",
       "  'learning_rate': 4.4710000000000004e-05,\n",
       "  'epoch': 5.29,\n",
       "  'step': 529},\n",
       " {'loss': 0.0139, 'learning_rate': 4.47e-05, 'epoch': 5.3, 'step': 530},\n",
       " {'loss': 0.0117, 'learning_rate': 4.469e-05, 'epoch': 5.31, 'step': 531},\n",
       " {'loss': 0.0003, 'learning_rate': 4.468e-05, 'epoch': 5.32, 'step': 532},\n",
       " {'loss': 0.3616, 'learning_rate': 4.467e-05, 'epoch': 5.33, 'step': 533},\n",
       " {'loss': 0.0786, 'learning_rate': 4.466e-05, 'epoch': 5.34, 'step': 534},\n",
       " {'loss': 0.0065, 'learning_rate': 4.465e-05, 'epoch': 5.35, 'step': 535},\n",
       " {'loss': 0.0856,\n",
       "  'learning_rate': 4.4640000000000006e-05,\n",
       "  'epoch': 5.36,\n",
       "  'step': 536},\n",
       " {'loss': 0.0256, 'learning_rate': 4.463e-05, 'epoch': 5.37, 'step': 537},\n",
       " {'loss': 0.0762, 'learning_rate': 4.462e-05, 'epoch': 5.38, 'step': 538},\n",
       " {'loss': 0.0047, 'learning_rate': 4.461e-05, 'epoch': 5.39, 'step': 539},\n",
       " {'loss': 0.0019, 'learning_rate': 4.46e-05, 'epoch': 5.4, 'step': 540},\n",
       " {'loss': 0.0275,\n",
       "  'learning_rate': 4.4590000000000005e-05,\n",
       "  'epoch': 5.41,\n",
       "  'step': 541},\n",
       " {'loss': 0.0025, 'learning_rate': 4.458e-05, 'epoch': 5.42, 'step': 542},\n",
       " {'loss': 0.0694, 'learning_rate': 4.457e-05, 'epoch': 5.43, 'step': 543},\n",
       " {'loss': 0.0064, 'learning_rate': 4.456e-05, 'epoch': 5.44, 'step': 544},\n",
       " {'loss': 0.0025,\n",
       "  'learning_rate': 4.4550000000000005e-05,\n",
       "  'epoch': 5.45,\n",
       "  'step': 545},\n",
       " {'loss': 0.0108,\n",
       "  'learning_rate': 4.4540000000000004e-05,\n",
       "  'epoch': 5.46,\n",
       "  'step': 546},\n",
       " {'loss': 0.0724, 'learning_rate': 4.453e-05, 'epoch': 5.47, 'step': 547},\n",
       " {'loss': 0.0552, 'learning_rate': 4.452e-05, 'epoch': 5.48, 'step': 548},\n",
       " {'loss': 0.0063, 'learning_rate': 4.451e-05, 'epoch': 5.49, 'step': 549},\n",
       " {'loss': 0.0004,\n",
       "  'learning_rate': 4.4500000000000004e-05,\n",
       "  'epoch': 5.5,\n",
       "  'step': 550},\n",
       " {'loss': 0.0608, 'learning_rate': 4.449e-05, 'epoch': 5.51, 'step': 551},\n",
       " {'loss': 0.0429, 'learning_rate': 4.448e-05, 'epoch': 5.52, 'step': 552},\n",
       " {'loss': 0.0029, 'learning_rate': 4.447e-05, 'epoch': 5.53, 'step': 553},\n",
       " {'loss': 0.0459,\n",
       "  'learning_rate': 4.4460000000000005e-05,\n",
       "  'epoch': 5.54,\n",
       "  'step': 554},\n",
       " {'loss': 0.0001, 'learning_rate': 4.445e-05, 'epoch': 5.55, 'step': 555},\n",
       " {'loss': 0.4813, 'learning_rate': 4.444e-05, 'epoch': 5.56, 'step': 556},\n",
       " {'loss': 0.0228, 'learning_rate': 4.443e-05, 'epoch': 5.57, 'step': 557},\n",
       " {'loss': 0.0067, 'learning_rate': 4.442e-05, 'epoch': 5.58, 'step': 558},\n",
       " {'loss': 0.0358,\n",
       "  'learning_rate': 4.4410000000000003e-05,\n",
       "  'epoch': 5.59,\n",
       "  'step': 559},\n",
       " {'loss': 0.7049, 'learning_rate': 4.44e-05, 'epoch': 5.6, 'step': 560},\n",
       " {'loss': 0.0037,\n",
       "  'learning_rate': 4.439000000000001e-05,\n",
       "  'epoch': 5.61,\n",
       "  'step': 561},\n",
       " {'loss': 0.0023, 'learning_rate': 4.438e-05, 'epoch': 5.62, 'step': 562},\n",
       " {'loss': 0.0003,\n",
       "  'learning_rate': 4.4370000000000004e-05,\n",
       "  'epoch': 5.63,\n",
       "  'step': 563},\n",
       " {'loss': 0.0002, 'learning_rate': 4.436e-05, 'epoch': 5.64, 'step': 564},\n",
       " {'loss': 0.1026, 'learning_rate': 4.435e-05, 'epoch': 5.65, 'step': 565},\n",
       " {'loss': 0.0041,\n",
       "  'learning_rate': 4.4340000000000006e-05,\n",
       "  'epoch': 5.66,\n",
       "  'step': 566},\n",
       " {'loss': 0.0002,\n",
       "  'learning_rate': 4.4330000000000004e-05,\n",
       "  'epoch': 5.67,\n",
       "  'step': 567},\n",
       " {'loss': 0.0364, 'learning_rate': 4.432e-05, 'epoch': 5.68, 'step': 568},\n",
       " {'loss': 0.0037, 'learning_rate': 4.431e-05, 'epoch': 5.69, 'step': 569},\n",
       " {'loss': 0.0058, 'learning_rate': 4.43e-05, 'epoch': 5.7, 'step': 570},\n",
       " {'loss': 0.0028,\n",
       "  'learning_rate': 4.4290000000000005e-05,\n",
       "  'epoch': 5.71,\n",
       "  'step': 571},\n",
       " {'loss': 0.0009, 'learning_rate': 4.428e-05, 'epoch': 5.72, 'step': 572},\n",
       " {'loss': 0.048, 'learning_rate': 4.427e-05, 'epoch': 5.73, 'step': 573},\n",
       " {'loss': 0.0717, 'learning_rate': 4.426e-05, 'epoch': 5.74, 'step': 574},\n",
       " {'loss': 0.4885,\n",
       "  'learning_rate': 4.4250000000000005e-05,\n",
       "  'epoch': 5.75,\n",
       "  'step': 575},\n",
       " {'loss': 0.1448, 'learning_rate': 4.424e-05, 'epoch': 5.76, 'step': 576},\n",
       " {'loss': 0.0056, 'learning_rate': 4.423e-05, 'epoch': 5.77, 'step': 577},\n",
       " {'loss': 0.6227, 'learning_rate': 4.422e-05, 'epoch': 5.78, 'step': 578},\n",
       " {'loss': 0.029, 'learning_rate': 4.421e-05, 'epoch': 5.79, 'step': 579},\n",
       " {'loss': 0.058,\n",
       "  'learning_rate': 4.4200000000000004e-05,\n",
       "  'epoch': 5.8,\n",
       "  'step': 580},\n",
       " {'loss': 0.022, 'learning_rate': 4.419e-05, 'epoch': 5.81, 'step': 581},\n",
       " {'loss': 0.2729,\n",
       "  'learning_rate': 4.418000000000001e-05,\n",
       "  'epoch': 5.82,\n",
       "  'step': 582},\n",
       " {'loss': 0.0012, 'learning_rate': 4.417e-05, 'epoch': 5.83, 'step': 583},\n",
       " {'loss': 0.0019,\n",
       "  'learning_rate': 4.4160000000000004e-05,\n",
       "  'epoch': 5.84,\n",
       "  'step': 584},\n",
       " {'loss': 0.0306, 'learning_rate': 4.415e-05, 'epoch': 5.85, 'step': 585},\n",
       " {'loss': 0.0094, 'learning_rate': 4.414e-05, 'epoch': 5.86, 'step': 586},\n",
       " {'loss': 0.2753,\n",
       "  'learning_rate': 4.4130000000000006e-05,\n",
       "  'epoch': 5.87,\n",
       "  'step': 587},\n",
       " {'loss': 0.0715, 'learning_rate': 4.412e-05, 'epoch': 5.88, 'step': 588},\n",
       " {'loss': 0.2969, 'learning_rate': 4.411e-05, 'epoch': 5.89, 'step': 589},\n",
       " {'loss': 0.0301, 'learning_rate': 4.41e-05, 'epoch': 5.9, 'step': 590},\n",
       " {'loss': 0.554,\n",
       "  'learning_rate': 4.4090000000000006e-05,\n",
       "  'epoch': 5.91,\n",
       "  'step': 591},\n",
       " {'loss': 0.1599,\n",
       "  'learning_rate': 4.4080000000000005e-05,\n",
       "  'epoch': 5.92,\n",
       "  'step': 592},\n",
       " {'loss': 0.0051, 'learning_rate': 4.407e-05, 'epoch': 5.93, 'step': 593},\n",
       " {'loss': 0.0053, 'learning_rate': 4.406e-05, 'epoch': 5.94, 'step': 594},\n",
       " {'loss': 0.0194, 'learning_rate': 4.405e-05, 'epoch': 5.95, 'step': 595},\n",
       " {'loss': 0.0198,\n",
       "  'learning_rate': 4.4040000000000005e-05,\n",
       "  'epoch': 5.96,\n",
       "  'step': 596},\n",
       " {'loss': 0.006,\n",
       "  'learning_rate': 4.4030000000000004e-05,\n",
       "  'epoch': 5.97,\n",
       "  'step': 597},\n",
       " {'loss': 0.0204, 'learning_rate': 4.402e-05, 'epoch': 5.98, 'step': 598},\n",
       " {'loss': 0.0268, 'learning_rate': 4.401e-05, 'epoch': 5.99, 'step': 599},\n",
       " {'loss': 0.0018,\n",
       "  'learning_rate': 4.4000000000000006e-05,\n",
       "  'epoch': 6.0,\n",
       "  'step': 600},\n",
       " {'eval_loss': 1.8639192581176758,\n",
       "  'eval_accuracy': 0.55,\n",
       "  'eval_runtime': 5.8947,\n",
       "  'eval_samples_per_second': 16.964,\n",
       "  'eval_steps_per_second': 2.205,\n",
       "  'epoch': 6.0,\n",
       "  'step': 600},\n",
       " {'loss': 0.0006,\n",
       "  'learning_rate': 4.3990000000000004e-05,\n",
       "  'epoch': 6.01,\n",
       "  'step': 601},\n",
       " {'loss': 0.0002, 'learning_rate': 4.398e-05, 'epoch': 6.02, 'step': 602},\n",
       " {'loss': 0.0154, 'learning_rate': 4.397e-05, 'epoch': 6.03, 'step': 603},\n",
       " {'loss': 0.0884, 'learning_rate': 4.396e-05, 'epoch': 6.04, 'step': 604},\n",
       " {'loss': 0.0002,\n",
       "  'learning_rate': 4.3950000000000004e-05,\n",
       "  'epoch': 6.05,\n",
       "  'step': 605},\n",
       " {'loss': 0.0013, 'learning_rate': 4.394e-05, 'epoch': 6.06, 'step': 606},\n",
       " {'loss': 0.0153, 'learning_rate': 4.393e-05, 'epoch': 6.07, 'step': 607},\n",
       " {'loss': 0.0006, 'learning_rate': 4.392e-05, 'epoch': 6.08, 'step': 608},\n",
       " {'loss': 0.1347, 'learning_rate': 4.391e-05, 'epoch': 6.09, 'step': 609},\n",
       " {'loss': 0.3765, 'learning_rate': 4.39e-05, 'epoch': 6.1, 'step': 610},\n",
       " {'loss': 0.0, 'learning_rate': 4.389e-05, 'epoch': 6.11, 'step': 611},\n",
       " {'loss': 0.0002,\n",
       "  'learning_rate': 4.388000000000001e-05,\n",
       "  'epoch': 6.12,\n",
       "  'step': 612},\n",
       " {'loss': 0.1738, 'learning_rate': 4.387e-05, 'epoch': 6.13, 'step': 613},\n",
       " {'loss': 0.0017,\n",
       "  'learning_rate': 4.3860000000000004e-05,\n",
       "  'epoch': 6.14,\n",
       "  'step': 614},\n",
       " {'loss': 0.0001, 'learning_rate': 4.385e-05, 'epoch': 6.15, 'step': 615},\n",
       " {'loss': 0.1124, 'learning_rate': 4.384e-05, 'epoch': 6.16, 'step': 616},\n",
       " {'loss': 0.1059,\n",
       "  'learning_rate': 4.3830000000000006e-05,\n",
       "  'epoch': 6.17,\n",
       "  'step': 617},\n",
       " {'loss': 0.0475, 'learning_rate': 4.382e-05, 'epoch': 6.18, 'step': 618},\n",
       " {'loss': 0.1047, 'learning_rate': 4.381e-05, 'epoch': 6.19, 'step': 619},\n",
       " {'loss': 0.1367, 'learning_rate': 4.38e-05, 'epoch': 6.2, 'step': 620},\n",
       " {'loss': 0.0,\n",
       "  'learning_rate': 4.3790000000000006e-05,\n",
       "  'epoch': 6.21,\n",
       "  'step': 621},\n",
       " {'loss': 0.695,\n",
       "  'learning_rate': 4.3780000000000004e-05,\n",
       "  'epoch': 6.22,\n",
       "  'step': 622},\n",
       " {'loss': 1.2283, 'learning_rate': 4.377e-05, 'epoch': 6.23, 'step': 623},\n",
       " {'loss': 0.0827, 'learning_rate': 4.376e-05, 'epoch': 6.24, 'step': 624},\n",
       " {'loss': 0.0005, 'learning_rate': 4.375e-05, 'epoch': 6.25, 'step': 625},\n",
       " {'loss': 1.4656,\n",
       "  'learning_rate': 4.3740000000000005e-05,\n",
       "  'epoch': 6.26,\n",
       "  'step': 626},\n",
       " {'loss': 0.091, 'learning_rate': 4.373e-05, 'epoch': 6.27, 'step': 627},\n",
       " {'loss': 0.4082, 'learning_rate': 4.372e-05, 'epoch': 6.28, 'step': 628},\n",
       " {'loss': 0.0058, 'learning_rate': 4.371e-05, 'epoch': 6.29, 'step': 629},\n",
       " {'loss': 0.0014,\n",
       "  'learning_rate': 4.3700000000000005e-05,\n",
       "  'epoch': 6.3,\n",
       "  'step': 630},\n",
       " {'loss': 0.0251,\n",
       "  'learning_rate': 4.3690000000000004e-05,\n",
       "  'epoch': 6.31,\n",
       "  'step': 631},\n",
       " {'loss': 0.0039, 'learning_rate': 4.368e-05, 'epoch': 6.32, 'step': 632},\n",
       " {'loss': 0.0067, 'learning_rate': 4.367e-05, 'epoch': 6.33, 'step': 633},\n",
       " {'loss': 0.2499, 'learning_rate': 4.366e-05, 'epoch': 6.34, 'step': 634},\n",
       " {'loss': 0.5938,\n",
       "  'learning_rate': 4.3650000000000004e-05,\n",
       "  'epoch': 6.35,\n",
       "  'step': 635},\n",
       " {'loss': 0.0356, 'learning_rate': 4.364e-05, 'epoch': 6.36, 'step': 636},\n",
       " {'loss': 0.0691,\n",
       "  'learning_rate': 4.363000000000001e-05,\n",
       "  'epoch': 6.37,\n",
       "  'step': 637},\n",
       " {'loss': 0.0767, 'learning_rate': 4.362e-05, 'epoch': 6.38, 'step': 638},\n",
       " {'loss': 0.0171, 'learning_rate': 4.361e-05, 'epoch': 6.39, 'step': 639},\n",
       " {'loss': 0.3027, 'learning_rate': 4.36e-05, 'epoch': 6.4, 'step': 640},\n",
       " {'loss': 0.0057, 'learning_rate': 4.359e-05, 'epoch': 6.41, 'step': 641},\n",
       " {'loss': 0.0043,\n",
       "  'learning_rate': 4.3580000000000006e-05,\n",
       "  'epoch': 6.42,\n",
       "  'step': 642},\n",
       " {'loss': 0.0027, 'learning_rate': 4.357e-05, 'epoch': 6.43, 'step': 643},\n",
       " {'loss': 0.0358, 'learning_rate': 4.356e-05, 'epoch': 6.44, 'step': 644},\n",
       " {'loss': 0.0188, 'learning_rate': 4.355e-05, 'epoch': 6.45, 'step': 645},\n",
       " {'loss': 0.0162, 'learning_rate': 4.354e-05, 'epoch': 6.46, 'step': 646},\n",
       " {'loss': 0.042,\n",
       "  'learning_rate': 4.3530000000000005e-05,\n",
       "  'epoch': 6.47,\n",
       "  'step': 647},\n",
       " {'loss': 0.1268, 'learning_rate': 4.352e-05, 'epoch': 6.48, 'step': 648},\n",
       " {'loss': 0.0437, 'learning_rate': 4.351e-05, 'epoch': 6.49, 'step': 649},\n",
       " {'loss': 0.0031, 'learning_rate': 4.35e-05, 'epoch': 6.5, 'step': 650},\n",
       " {'loss': 0.0351,\n",
       "  'learning_rate': 4.3490000000000005e-05,\n",
       "  'epoch': 6.51,\n",
       "  'step': 651},\n",
       " {'loss': 0.003,\n",
       "  'learning_rate': 4.3480000000000004e-05,\n",
       "  'epoch': 6.52,\n",
       "  'step': 652},\n",
       " {'loss': 0.0023, 'learning_rate': 4.347e-05, 'epoch': 6.53, 'step': 653},\n",
       " {'loss': 0.0007, 'learning_rate': 4.346e-05, 'epoch': 6.54, 'step': 654},\n",
       " {'loss': 0.0014, 'learning_rate': 4.345e-05, 'epoch': 6.55, 'step': 655},\n",
       " {'loss': 0.0004,\n",
       "  'learning_rate': 4.3440000000000004e-05,\n",
       "  'epoch': 6.56,\n",
       "  'step': 656},\n",
       " {'loss': 0.0174, 'learning_rate': 4.343e-05, 'epoch': 6.57, 'step': 657},\n",
       " {'loss': 0.0011, 'learning_rate': 4.342e-05, 'epoch': 6.58, 'step': 658},\n",
       " {'loss': 0.007, 'learning_rate': 4.341e-05, 'epoch': 6.59, 'step': 659},\n",
       " {'loss': 0.0004,\n",
       "  'learning_rate': 4.3400000000000005e-05,\n",
       "  'epoch': 6.6,\n",
       "  'step': 660},\n",
       " {'loss': 0.0036, 'learning_rate': 4.339e-05, 'epoch': 6.61, 'step': 661},\n",
       " {'loss': 0.0259, 'learning_rate': 4.338e-05, 'epoch': 6.62, 'step': 662},\n",
       " {'loss': 0.0025, 'learning_rate': 4.337e-05, 'epoch': 6.63, 'step': 663},\n",
       " {'loss': 0.064, 'learning_rate': 4.336e-05, 'epoch': 6.64, 'step': 664},\n",
       " {'loss': 0.0021, 'learning_rate': 4.335e-05, 'epoch': 6.65, 'step': 665},\n",
       " {'loss': 0.0009, 'learning_rate': 4.334e-05, 'epoch': 6.66, 'step': 666},\n",
       " {'loss': 0.0235,\n",
       "  'learning_rate': 4.333000000000001e-05,\n",
       "  'epoch': 6.67,\n",
       "  'step': 667},\n",
       " {'loss': 0.0003, 'learning_rate': 4.332e-05, 'epoch': 6.68, 'step': 668},\n",
       " {'loss': 1.523,\n",
       "  'learning_rate': 4.3310000000000004e-05,\n",
       "  'epoch': 6.69,\n",
       "  'step': 669},\n",
       " {'loss': 0.0008, 'learning_rate': 4.33e-05, 'epoch': 6.7, 'step': 670},\n",
       " {'loss': 0.0094, 'learning_rate': 4.329e-05, 'epoch': 6.71, 'step': 671},\n",
       " {'loss': 0.0001,\n",
       "  'learning_rate': 4.3280000000000006e-05,\n",
       "  'epoch': 6.72,\n",
       "  'step': 672},\n",
       " {'loss': 0.0894, 'learning_rate': 4.327e-05, 'epoch': 6.73, 'step': 673},\n",
       " {'loss': 0.0038, 'learning_rate': 4.326e-05, 'epoch': 6.74, 'step': 674},\n",
       " {'loss': 0.0003, 'learning_rate': 4.325e-05, 'epoch': 6.75, 'step': 675},\n",
       " {'loss': 0.0016, 'learning_rate': 4.324e-05, 'epoch': 6.76, 'step': 676},\n",
       " {'loss': 0.0037,\n",
       "  'learning_rate': 4.3230000000000005e-05,\n",
       "  'epoch': 6.77,\n",
       "  'step': 677},\n",
       " {'loss': 0.0003,\n",
       "  'learning_rate': 4.3219999999999996e-05,\n",
       "  'epoch': 6.78,\n",
       "  'step': 678},\n",
       " {'loss': 0.2239, 'learning_rate': 4.321e-05, 'epoch': 6.79, 'step': 679},\n",
       " {'loss': 0.0017, 'learning_rate': 4.32e-05, 'epoch': 6.8, 'step': 680},\n",
       " {'loss': 1.2227,\n",
       "  'learning_rate': 4.3190000000000005e-05,\n",
       "  'epoch': 6.81,\n",
       "  'step': 681},\n",
       " {'loss': 0.0074, 'learning_rate': 4.318e-05, 'epoch': 6.82, 'step': 682},\n",
       " {'loss': 0.0005, 'learning_rate': 4.317e-05, 'epoch': 6.83, 'step': 683},\n",
       " {'loss': 0.0035, 'learning_rate': 4.316e-05, 'epoch': 6.84, 'step': 684},\n",
       " {'loss': 0.0093, 'learning_rate': 4.315e-05, 'epoch': 6.85, 'step': 685},\n",
       " {'loss': 0.0271,\n",
       "  'learning_rate': 4.3140000000000004e-05,\n",
       "  'epoch': 6.86,\n",
       "  'step': 686},\n",
       " {'loss': 0.0033, 'learning_rate': 4.313e-05, 'epoch': 6.87, 'step': 687},\n",
       " {'loss': 0.019,\n",
       "  'learning_rate': 4.312000000000001e-05,\n",
       "  'epoch': 6.88,\n",
       "  'step': 688},\n",
       " {'loss': 0.0016, 'learning_rate': 4.311e-05, 'epoch': 6.89, 'step': 689},\n",
       " {'loss': 0.0297,\n",
       "  'learning_rate': 4.3100000000000004e-05,\n",
       "  'epoch': 6.9,\n",
       "  'step': 690},\n",
       " {'loss': 0.0291, 'learning_rate': 4.309e-05, 'epoch': 6.91, 'step': 691},\n",
       " {'loss': 0.0014, 'learning_rate': 4.308e-05, 'epoch': 6.92, 'step': 692},\n",
       " {'loss': 0.0011,\n",
       "  'learning_rate': 4.3070000000000006e-05,\n",
       "  'epoch': 6.93,\n",
       "  'step': 693},\n",
       " {'loss': 0.0027, 'learning_rate': 4.306e-05, 'epoch': 6.94, 'step': 694},\n",
       " {'loss': 0.0019, 'learning_rate': 4.305e-05, 'epoch': 6.95, 'step': 695},\n",
       " {'loss': 0.0018, 'learning_rate': 4.304e-05, 'epoch': 6.96, 'step': 696},\n",
       " {'loss': 0.0005,\n",
       "  'learning_rate': 4.3030000000000006e-05,\n",
       "  'epoch': 6.97,\n",
       "  'step': 697},\n",
       " {'loss': 0.0013,\n",
       "  'learning_rate': 4.3020000000000005e-05,\n",
       "  'epoch': 6.98,\n",
       "  'step': 698},\n",
       " {'loss': 0.0009, 'learning_rate': 4.301e-05, 'epoch': 6.99, 'step': 699},\n",
       " {'loss': 0.002, 'learning_rate': 4.3e-05, 'epoch': 7.0, 'step': 700},\n",
       " {'eval_loss': 2.20063853263855,\n",
       "  'eval_accuracy': 0.52,\n",
       "  'eval_runtime': 5.8943,\n",
       "  'eval_samples_per_second': 16.965,\n",
       "  'eval_steps_per_second': 2.206,\n",
       "  'epoch': 7.0,\n",
       "  'step': 700},\n",
       " {'loss': 0.0019, 'learning_rate': 4.299e-05, 'epoch': 7.01, 'step': 701},\n",
       " {'loss': 0.0641,\n",
       "  'learning_rate': 4.2980000000000005e-05,\n",
       "  'epoch': 7.02,\n",
       "  'step': 702},\n",
       " {'loss': 0.6106,\n",
       "  'learning_rate': 4.2970000000000004e-05,\n",
       "  'epoch': 7.03,\n",
       "  'step': 703},\n",
       " {'loss': 0.0001, 'learning_rate': 4.296e-05, 'epoch': 7.04, 'step': 704},\n",
       " {'loss': 0.003, 'learning_rate': 4.295e-05, 'epoch': 7.05, 'step': 705},\n",
       " {'loss': 0.0042,\n",
       "  'learning_rate': 4.2940000000000006e-05,\n",
       "  'epoch': 7.06,\n",
       "  'step': 706},\n",
       " {'loss': 0.0001,\n",
       "  'learning_rate': 4.2930000000000004e-05,\n",
       "  'epoch': 7.07,\n",
       "  'step': 707},\n",
       " {'loss': 0.0001, 'learning_rate': 4.292e-05, 'epoch': 7.08, 'step': 708},\n",
       " {'loss': 0.0001, 'learning_rate': 4.291e-05, 'epoch': 7.09, 'step': 709},\n",
       " {'loss': 0.0001, 'learning_rate': 4.29e-05, 'epoch': 7.1, 'step': 710},\n",
       " {'loss': 0.0002,\n",
       "  'learning_rate': 4.2890000000000004e-05,\n",
       "  'epoch': 7.11,\n",
       "  'step': 711},\n",
       " {'loss': 0.9697, 'learning_rate': 4.288e-05, 'epoch': 7.12, 'step': 712},\n",
       " {'loss': 0.0528,\n",
       "  'learning_rate': 4.287000000000001e-05,\n",
       "  'epoch': 7.13,\n",
       "  'step': 713},\n",
       " {'loss': 0.0001, 'learning_rate': 4.286e-05, 'epoch': 7.14, 'step': 714},\n",
       " {'loss': 0.0002, 'learning_rate': 4.285e-05, 'epoch': 7.15, 'step': 715},\n",
       " {'loss': 0.0042, 'learning_rate': 4.284e-05, 'epoch': 7.16, 'step': 716},\n",
       " {'loss': 0.0015, 'learning_rate': 4.283e-05, 'epoch': 7.17, 'step': 717},\n",
       " {'loss': 0.0003,\n",
       "  'learning_rate': 4.282000000000001e-05,\n",
       "  'epoch': 7.18,\n",
       "  'step': 718},\n",
       " {'loss': 0.0722, 'learning_rate': 4.281e-05, 'epoch': 7.19, 'step': 719},\n",
       " {'loss': 0.0334,\n",
       "  'learning_rate': 4.2800000000000004e-05,\n",
       "  'epoch': 7.2,\n",
       "  'step': 720},\n",
       " {'loss': 0.0002, 'learning_rate': 4.279e-05, 'epoch': 7.21, 'step': 721},\n",
       " {'loss': 0.0237, 'learning_rate': 4.278e-05, 'epoch': 7.22, 'step': 722},\n",
       " {'loss': 0.2777,\n",
       "  'learning_rate': 4.2770000000000006e-05,\n",
       "  'epoch': 7.23,\n",
       "  'step': 723},\n",
       " {'loss': 0.364, 'learning_rate': 4.276e-05, 'epoch': 7.24, 'step': 724},\n",
       " {'loss': 0.0006, 'learning_rate': 4.275e-05, 'epoch': 7.25, 'step': 725},\n",
       " {'loss': 0.002, 'learning_rate': 4.274e-05, 'epoch': 7.26, 'step': 726},\n",
       " {'loss': 0.0018,\n",
       "  'learning_rate': 4.2730000000000006e-05,\n",
       "  'epoch': 7.27,\n",
       "  'step': 727},\n",
       " {'loss': 0.0017,\n",
       "  'learning_rate': 4.2720000000000004e-05,\n",
       "  'epoch': 7.28,\n",
       "  'step': 728},\n",
       " {'loss': 0.0024, 'learning_rate': 4.271e-05, 'epoch': 7.29, 'step': 729},\n",
       " {'loss': 0.0101, 'learning_rate': 4.27e-05, 'epoch': 7.3, 'step': 730},\n",
       " {'loss': 0.0041, 'learning_rate': 4.269e-05, 'epoch': 7.31, 'step': 731},\n",
       " {'loss': 0.2303,\n",
       "  'learning_rate': 4.2680000000000005e-05,\n",
       "  'epoch': 7.32,\n",
       "  'step': 732},\n",
       " {'loss': 0.1219, 'learning_rate': 4.267e-05, 'epoch': 7.33, 'step': 733},\n",
       " {'loss': 0.0043, 'learning_rate': 4.266e-05, 'epoch': 7.34, 'step': 734},\n",
       " {'loss': 0.002, 'learning_rate': 4.265e-05, 'epoch': 7.35, 'step': 735},\n",
       " {'loss': 0.0011,\n",
       "  'learning_rate': 4.2640000000000005e-05,\n",
       "  'epoch': 7.36,\n",
       "  'step': 736},\n",
       " {'loss': 0.0033,\n",
       "  'learning_rate': 4.2630000000000004e-05,\n",
       "  'epoch': 7.37,\n",
       "  'step': 737},\n",
       " {'loss': 0.0393, 'learning_rate': 4.262e-05, 'epoch': 7.38, 'step': 738},\n",
       " {'loss': 0.0037, 'learning_rate': 4.261e-05, 'epoch': 7.39, 'step': 739},\n",
       " {'loss': 0.0922, 'learning_rate': 4.26e-05, 'epoch': 7.4, 'step': 740},\n",
       " {'loss': 0.0064,\n",
       "  'learning_rate': 4.2590000000000004e-05,\n",
       "  'epoch': 7.41,\n",
       "  'step': 741},\n",
       " {'loss': 0.0035, 'learning_rate': 4.258e-05, 'epoch': 7.42, 'step': 742},\n",
       " {'loss': 0.0239,\n",
       "  'learning_rate': 4.257000000000001e-05,\n",
       "  'epoch': 7.43,\n",
       "  'step': 743},\n",
       " {'loss': 0.0005, 'learning_rate': 4.256e-05, 'epoch': 7.44, 'step': 744},\n",
       " {'loss': 0.0012,\n",
       "  'learning_rate': 4.2550000000000004e-05,\n",
       "  'epoch': 7.45,\n",
       "  'step': 745},\n",
       " {'loss': 0.0029, 'learning_rate': 4.254e-05, 'epoch': 7.46, 'step': 746},\n",
       " {'loss': 0.0005, 'learning_rate': 4.253e-05, 'epoch': 7.47, 'step': 747},\n",
       " {'loss': 0.0258,\n",
       "  'learning_rate': 4.2520000000000006e-05,\n",
       "  'epoch': 7.48,\n",
       "  'step': 748},\n",
       " {'loss': 0.0024, 'learning_rate': 4.251e-05, 'epoch': 7.49, 'step': 749},\n",
       " {'loss': 0.0007, 'learning_rate': 4.25e-05, 'epoch': 7.5, 'step': 750},\n",
       " {'loss': 0.0061, 'learning_rate': 4.249e-05, 'epoch': 7.51, 'step': 751},\n",
       " {'loss': 0.7038, 'learning_rate': 4.248e-05, 'epoch': 7.52, 'step': 752},\n",
       " {'loss': 0.1379,\n",
       "  'learning_rate': 4.2470000000000005e-05,\n",
       "  'epoch': 7.53,\n",
       "  'step': 753},\n",
       " {'loss': 0.0013, 'learning_rate': 4.246e-05, 'epoch': 7.54, 'step': 754},\n",
       " {'loss': 0.0038, 'learning_rate': 4.245e-05, 'epoch': 7.55, 'step': 755},\n",
       " {'loss': 0.0042, 'learning_rate': 4.244e-05, 'epoch': 7.56, 'step': 756},\n",
       " {'loss': 0.0141,\n",
       "  'learning_rate': 4.2430000000000005e-05,\n",
       "  'epoch': 7.57,\n",
       "  'step': 757},\n",
       " {'loss': 0.0066,\n",
       "  'learning_rate': 4.2420000000000004e-05,\n",
       "  'epoch': 7.58,\n",
       "  'step': 758},\n",
       " {'loss': 0.0459, 'learning_rate': 4.241e-05, 'epoch': 7.59, 'step': 759},\n",
       " {'loss': 0.2153, 'learning_rate': 4.24e-05, 'epoch': 7.6, 'step': 760},\n",
       " {'loss': 0.0008, 'learning_rate': 4.239e-05, 'epoch': 7.61, 'step': 761},\n",
       " {'loss': 0.5716,\n",
       "  'learning_rate': 4.2380000000000004e-05,\n",
       "  'epoch': 7.62,\n",
       "  'step': 762},\n",
       " {'loss': 0.0071, 'learning_rate': 4.237e-05, 'epoch': 7.63, 'step': 763},\n",
       " {'loss': 0.078, 'learning_rate': 4.236e-05, 'epoch': 7.64, 'step': 764},\n",
       " {'loss': 0.2222, 'learning_rate': 4.235e-05, 'epoch': 7.65, 'step': 765},\n",
       " {'loss': 0.0002,\n",
       "  'learning_rate': 4.2340000000000005e-05,\n",
       "  'epoch': 7.66,\n",
       "  'step': 766},\n",
       " {'loss': 0.0003, 'learning_rate': 4.233e-05, 'epoch': 7.67, 'step': 767},\n",
       " {'loss': 0.0012, 'learning_rate': 4.232e-05, 'epoch': 7.68, 'step': 768},\n",
       " {'loss': 0.0564, 'learning_rate': 4.231e-05, 'epoch': 7.69, 'step': 769},\n",
       " {'loss': 0.0582, 'learning_rate': 4.23e-05, 'epoch': 7.7, 'step': 770},\n",
       " {'loss': 0.0001, 'learning_rate': 4.229e-05, 'epoch': 7.71, 'step': 771},\n",
       " {'loss': 0.0007, 'learning_rate': 4.228e-05, 'epoch': 7.72, 'step': 772},\n",
       " {'loss': 0.7918,\n",
       "  'learning_rate': 4.227000000000001e-05,\n",
       "  'epoch': 7.73,\n",
       "  'step': 773},\n",
       " {'loss': 0.0001, 'learning_rate': 4.226e-05, 'epoch': 7.74, 'step': 774},\n",
       " {'loss': 0.0012,\n",
       "  'learning_rate': 4.2250000000000004e-05,\n",
       "  'epoch': 7.75,\n",
       "  'step': 775},\n",
       " {'loss': 0.0024, 'learning_rate': 4.224e-05, 'epoch': 7.76, 'step': 776},\n",
       " {'loss': 0.0017, 'learning_rate': 4.223e-05, 'epoch': 7.77, 'step': 777},\n",
       " {'loss': 0.0075,\n",
       "  'learning_rate': 4.2220000000000006e-05,\n",
       "  'epoch': 7.78,\n",
       "  'step': 778},\n",
       " {'loss': 0.4663, 'learning_rate': 4.221e-05, 'epoch': 7.79, 'step': 779},\n",
       " {'loss': 0.0385, 'learning_rate': 4.22e-05, 'epoch': 7.8, 'step': 780},\n",
       " {'loss': 0.0006, 'learning_rate': 4.219e-05, 'epoch': 7.81, 'step': 781},\n",
       " {'loss': 0.0022,\n",
       "  'learning_rate': 4.2180000000000006e-05,\n",
       "  'epoch': 7.82,\n",
       "  'step': 782},\n",
       " {'loss': 0.0047,\n",
       "  'learning_rate': 4.2170000000000005e-05,\n",
       "  'epoch': 7.83,\n",
       "  'step': 783},\n",
       " {'loss': 0.0723,\n",
       "  'learning_rate': 4.2159999999999996e-05,\n",
       "  'epoch': 7.84,\n",
       "  'step': 784},\n",
       " {'loss': 0.0035, 'learning_rate': 4.215e-05, 'epoch': 7.85, 'step': 785},\n",
       " {'loss': 0.0196, 'learning_rate': 4.214e-05, 'epoch': 7.86, 'step': 786},\n",
       " {'loss': 0.0031,\n",
       "  'learning_rate': 4.2130000000000005e-05,\n",
       "  'epoch': 7.87,\n",
       "  'step': 787},\n",
       " {'loss': 0.0006, 'learning_rate': 4.212e-05, 'epoch': 7.88, 'step': 788},\n",
       " {'loss': 0.0012, 'learning_rate': 4.211e-05, 'epoch': 7.89, 'step': 789},\n",
       " {'loss': 0.0004, 'learning_rate': 4.21e-05, 'epoch': 7.9, 'step': 790},\n",
       " {'loss': 0.0261, 'learning_rate': 4.209e-05, 'epoch': 7.91, 'step': 791},\n",
       " {'loss': 0.4024,\n",
       "  'learning_rate': 4.2080000000000004e-05,\n",
       "  'epoch': 7.92,\n",
       "  'step': 792},\n",
       " {'loss': 0.018, 'learning_rate': 4.207e-05, 'epoch': 7.93, 'step': 793},\n",
       " {'loss': 0.0118, 'learning_rate': 4.206e-05, 'epoch': 7.94, 'step': 794},\n",
       " {'loss': 0.0028, 'learning_rate': 4.205e-05, 'epoch': 7.95, 'step': 795},\n",
       " {'loss': 0.0005,\n",
       "  'learning_rate': 4.2040000000000004e-05,\n",
       "  'epoch': 7.96,\n",
       "  'step': 796},\n",
       " {'loss': 0.0009, 'learning_rate': 4.203e-05, 'epoch': 7.97, 'step': 797},\n",
       " {'loss': 0.0017, 'learning_rate': 4.202e-05, 'epoch': 7.98, 'step': 798},\n",
       " {'loss': 0.0334, 'learning_rate': 4.201e-05, 'epoch': 7.99, 'step': 799},\n",
       " {'loss': 0.0037, 'learning_rate': 4.2e-05, 'epoch': 8.0, 'step': 800},\n",
       " {'eval_loss': 1.718042254447937,\n",
       "  'eval_accuracy': 0.55,\n",
       "  'eval_runtime': 5.8829,\n",
       "  'eval_samples_per_second': 16.998,\n",
       "  'eval_steps_per_second': 2.21,\n",
       "  'epoch': 8.0,\n",
       "  'step': 800},\n",
       " {'loss': 0.0062, 'learning_rate': 4.199e-05, 'epoch': 8.01, 'step': 801},\n",
       " {'loss': 0.0002, 'learning_rate': 4.198e-05, 'epoch': 8.02, 'step': 802},\n",
       " {'loss': 0.013,\n",
       "  'learning_rate': 4.1970000000000006e-05,\n",
       "  'epoch': 8.03,\n",
       "  'step': 803},\n",
       " {'loss': 0.0005, 'learning_rate': 4.196e-05, 'epoch': 8.04, 'step': 804},\n",
       " {'loss': 0.0248, 'learning_rate': 4.195e-05, 'epoch': 8.05, 'step': 805},\n",
       " {'loss': 0.0001, 'learning_rate': 4.194e-05, 'epoch': 8.06, 'step': 806},\n",
       " {'loss': 0.0007, 'learning_rate': 4.193e-05, 'epoch': 8.07, 'step': 807},\n",
       " {'loss': 0.2207,\n",
       "  'learning_rate': 4.1920000000000005e-05,\n",
       "  'epoch': 8.08,\n",
       "  'step': 808},\n",
       " {'loss': 0.0008, 'learning_rate': 4.191e-05, 'epoch': 8.09, 'step': 809},\n",
       " {'loss': 0.0, 'learning_rate': 4.19e-05, 'epoch': 8.1, 'step': 810},\n",
       " {'loss': 0.0009, 'learning_rate': 4.189e-05, 'epoch': 8.11, 'step': 811},\n",
       " {'loss': 0.0001,\n",
       "  'learning_rate': 4.1880000000000006e-05,\n",
       "  'epoch': 8.12,\n",
       "  'step': 812},\n",
       " {'loss': 0.0001,\n",
       "  'learning_rate': 4.1870000000000004e-05,\n",
       "  'epoch': 8.13,\n",
       "  'step': 813},\n",
       " {'loss': 0.0001, 'learning_rate': 4.186e-05, 'epoch': 8.14, 'step': 814},\n",
       " {'loss': 0.0003, 'learning_rate': 4.185e-05, 'epoch': 8.15, 'step': 815},\n",
       " {'loss': 0.0001, 'learning_rate': 4.184e-05, 'epoch': 8.16, 'step': 816},\n",
       " {'loss': 0.0015,\n",
       "  'learning_rate': 4.1830000000000004e-05,\n",
       "  'epoch': 8.17,\n",
       "  'step': 817},\n",
       " {'loss': 0.0, 'learning_rate': 4.182e-05, 'epoch': 8.18, 'step': 818},\n",
       " {'loss': 0.0051,\n",
       "  'learning_rate': 4.181000000000001e-05,\n",
       "  'epoch': 8.19,\n",
       "  'step': 819},\n",
       " {'loss': 0.0544, 'learning_rate': 4.18e-05, 'epoch': 8.2, 'step': 820},\n",
       " {'loss': 0.0001, 'learning_rate': 4.179e-05, 'epoch': 8.21, 'step': 821},\n",
       " {'loss': 0.0001, 'learning_rate': 4.178e-05, 'epoch': 8.22, 'step': 822},\n",
       " {'loss': 0.0004, 'learning_rate': 4.177e-05, 'epoch': 8.23, 'step': 823},\n",
       " {'loss': 0.0009,\n",
       "  'learning_rate': 4.176000000000001e-05,\n",
       "  'epoch': 8.24,\n",
       "  'step': 824},\n",
       " {'loss': 0.0189, 'learning_rate': 4.175e-05, 'epoch': 8.25, 'step': 825},\n",
       " {'loss': 0.1222,\n",
       "  'learning_rate': 4.1740000000000004e-05,\n",
       "  'epoch': 8.26,\n",
       "  'step': 826},\n",
       " {'loss': 0.4753, 'learning_rate': 4.173e-05, 'epoch': 8.27, 'step': 827},\n",
       " {'loss': 0.0003, 'learning_rate': 4.172e-05, 'epoch': 8.28, 'step': 828},\n",
       " {'loss': 0.3326,\n",
       "  'learning_rate': 4.1710000000000006e-05,\n",
       "  'epoch': 8.29,\n",
       "  'step': 829},\n",
       " {'loss': 0.0026, 'learning_rate': 4.17e-05, 'epoch': 8.3, 'step': 830},\n",
       " {'loss': 0.0006, 'learning_rate': 4.169e-05, 'epoch': 8.31, 'step': 831},\n",
       " {'loss': 0.0245, 'learning_rate': 4.168e-05, 'epoch': 8.32, 'step': 832},\n",
       " {'loss': 0.0075,\n",
       "  'learning_rate': 4.1670000000000006e-05,\n",
       "  'epoch': 8.33,\n",
       "  'step': 833},\n",
       " {'loss': 0.0041,\n",
       "  'learning_rate': 4.1660000000000004e-05,\n",
       "  'epoch': 8.34,\n",
       "  'step': 834},\n",
       " {'loss': 0.0267, 'learning_rate': 4.165e-05, 'epoch': 8.35, 'step': 835},\n",
       " {'loss': 0.6474, 'learning_rate': 4.164e-05, 'epoch': 8.36, 'step': 836},\n",
       " {'loss': 0.0047, 'learning_rate': 4.163e-05, 'epoch': 8.37, 'step': 837},\n",
       " {'loss': 0.0168,\n",
       "  'learning_rate': 4.1620000000000005e-05,\n",
       "  'epoch': 8.38,\n",
       "  'step': 838},\n",
       " {'loss': 0.0012, 'learning_rate': 4.161e-05, 'epoch': 8.39, 'step': 839},\n",
       " {'loss': 0.0014, 'learning_rate': 4.16e-05, 'epoch': 8.4, 'step': 840},\n",
       " {'loss': 0.0044, 'learning_rate': 4.159e-05, 'epoch': 8.41, 'step': 841},\n",
       " {'loss': 0.1408,\n",
       "  'learning_rate': 4.1580000000000005e-05,\n",
       "  'epoch': 8.42,\n",
       "  'step': 842},\n",
       " {'loss': 0.0011,\n",
       "  'learning_rate': 4.1570000000000003e-05,\n",
       "  'epoch': 8.43,\n",
       "  'step': 843},\n",
       " {'loss': 0.0008, 'learning_rate': 4.156e-05, 'epoch': 8.44, 'step': 844},\n",
       " {'loss': 0.4849, 'learning_rate': 4.155e-05, 'epoch': 8.45, 'step': 845},\n",
       " {'loss': 0.0013, 'learning_rate': 4.154e-05, 'epoch': 8.46, 'step': 846},\n",
       " {'loss': 0.001,\n",
       "  'learning_rate': 4.1530000000000004e-05,\n",
       "  'epoch': 8.47,\n",
       "  'step': 847},\n",
       " {'loss': 0.0068, 'learning_rate': 4.152e-05, 'epoch': 8.48, 'step': 848},\n",
       " {'loss': 0.0005,\n",
       "  'learning_rate': 4.151000000000001e-05,\n",
       "  'epoch': 8.49,\n",
       "  'step': 849},\n",
       " {'loss': 0.0005, 'learning_rate': 4.15e-05, 'epoch': 8.5, 'step': 850},\n",
       " {'loss': 0.0014,\n",
       "  'learning_rate': 4.1490000000000004e-05,\n",
       "  'epoch': 8.51,\n",
       "  'step': 851},\n",
       " {'loss': 0.0002, 'learning_rate': 4.148e-05, 'epoch': 8.52, 'step': 852},\n",
       " {'loss': 0.0028, 'learning_rate': 4.147e-05, 'epoch': 8.53, 'step': 853},\n",
       " {'loss': 0.0004,\n",
       "  'learning_rate': 4.1460000000000006e-05,\n",
       "  'epoch': 8.54,\n",
       "  'step': 854},\n",
       " {'loss': 0.0746, 'learning_rate': 4.145e-05, 'epoch': 8.55, 'step': 855},\n",
       " {'loss': 0.0021, 'learning_rate': 4.144e-05, 'epoch': 8.56, 'step': 856},\n",
       " {'loss': 0.0049, 'learning_rate': 4.143e-05, 'epoch': 8.57, 'step': 857},\n",
       " {'loss': 0.0023,\n",
       "  'learning_rate': 4.142000000000001e-05,\n",
       "  'epoch': 8.58,\n",
       "  'step': 858},\n",
       " {'loss': 0.0598,\n",
       "  'learning_rate': 4.1410000000000005e-05,\n",
       "  'epoch': 8.59,\n",
       "  'step': 859},\n",
       " {'loss': 0.0088, 'learning_rate': 4.14e-05, 'epoch': 8.6, 'step': 860},\n",
       " {'loss': 0.0545, 'learning_rate': 4.139e-05, 'epoch': 8.61, 'step': 861},\n",
       " {'loss': 0.0162, 'learning_rate': 4.138e-05, 'epoch': 8.62, 'step': 862},\n",
       " {'loss': 0.0001,\n",
       "  'learning_rate': 4.1370000000000005e-05,\n",
       "  'epoch': 8.63,\n",
       "  'step': 863},\n",
       " {'loss': 0.001,\n",
       "  'learning_rate': 4.1360000000000004e-05,\n",
       "  'epoch': 8.64,\n",
       "  'step': 864},\n",
       " {'loss': 0.0006, 'learning_rate': 4.135e-05, 'epoch': 8.65, 'step': 865},\n",
       " {'loss': 0.0004, 'learning_rate': 4.134e-05, 'epoch': 8.66, 'step': 866},\n",
       " {'loss': 0.0001, 'learning_rate': 4.133e-05, 'epoch': 8.67, 'step': 867},\n",
       " {'loss': 0.0001,\n",
       "  'learning_rate': 4.1320000000000004e-05,\n",
       "  'epoch': 8.68,\n",
       "  'step': 868},\n",
       " {'loss': 0.0064, 'learning_rate': 4.131e-05, 'epoch': 8.69, 'step': 869},\n",
       " {'loss': 0.0004, 'learning_rate': 4.13e-05, 'epoch': 8.7, 'step': 870},\n",
       " {'loss': 0.003, 'learning_rate': 4.129e-05, 'epoch': 8.71, 'step': 871},\n",
       " {'loss': 0.0,\n",
       "  'learning_rate': 4.1280000000000005e-05,\n",
       "  'epoch': 8.72,\n",
       "  'step': 872},\n",
       " {'loss': 0.0001, 'learning_rate': 4.127e-05, 'epoch': 8.73, 'step': 873},\n",
       " {'loss': 0.012, 'learning_rate': 4.126e-05, 'epoch': 8.74, 'step': 874},\n",
       " {'loss': 0.1798, 'learning_rate': 4.125e-05, 'epoch': 8.75, 'step': 875},\n",
       " {'loss': 0.0005, 'learning_rate': 4.124e-05, 'epoch': 8.76, 'step': 876},\n",
       " {'loss': 0.0012, 'learning_rate': 4.123e-05, 'epoch': 8.77, 'step': 877},\n",
       " {'loss': 1.5076, 'learning_rate': 4.122e-05, 'epoch': 8.78, 'step': 878},\n",
       " {'loss': 0.0003,\n",
       "  'learning_rate': 4.121000000000001e-05,\n",
       "  'epoch': 8.79,\n",
       "  'step': 879},\n",
       " {'loss': 0.0007, 'learning_rate': 4.12e-05, 'epoch': 8.8, 'step': 880},\n",
       " {'loss': 0.0048,\n",
       "  'learning_rate': 4.1190000000000004e-05,\n",
       "  'epoch': 8.81,\n",
       "  'step': 881},\n",
       " {'loss': 0.0029, 'learning_rate': 4.118e-05, 'epoch': 8.82, 'step': 882},\n",
       " {'loss': 0.0527, 'learning_rate': 4.117e-05, 'epoch': 8.83, 'step': 883},\n",
       " {'loss': 0.001,\n",
       "  'learning_rate': 4.1160000000000006e-05,\n",
       "  'epoch': 8.84,\n",
       "  'step': 884},\n",
       " {'loss': 0.003, 'learning_rate': 4.115e-05, 'epoch': 8.85, 'step': 885},\n",
       " {'loss': 0.0303, 'learning_rate': 4.114e-05, 'epoch': 8.86, 'step': 886},\n",
       " {'loss': 0.0013, 'learning_rate': 4.113e-05, 'epoch': 8.87, 'step': 887},\n",
       " {'loss': 0.0011,\n",
       "  'learning_rate': 4.1120000000000006e-05,\n",
       "  'epoch': 8.88,\n",
       "  'step': 888},\n",
       " {'loss': 0.0006,\n",
       "  'learning_rate': 4.1110000000000005e-05,\n",
       "  'epoch': 8.89,\n",
       "  'step': 889},\n",
       " {'loss': 0.6821, 'learning_rate': 4.11e-05, 'epoch': 8.9, 'step': 890},\n",
       " {'loss': 0.001, 'learning_rate': 4.109e-05, 'epoch': 8.91, 'step': 891},\n",
       " {'loss': 0.0492, 'learning_rate': 4.108e-05, 'epoch': 8.92, 'step': 892},\n",
       " {'loss': 0.0013,\n",
       "  'learning_rate': 4.1070000000000005e-05,\n",
       "  'epoch': 8.93,\n",
       "  'step': 893},\n",
       " {'loss': 0.6948, 'learning_rate': 4.106e-05, 'epoch': 8.94, 'step': 894},\n",
       " {'loss': 0.6526, 'learning_rate': 4.105e-05, 'epoch': 8.95, 'step': 895},\n",
       " {'loss': 0.0007, 'learning_rate': 4.104e-05, 'epoch': 8.96, 'step': 896},\n",
       " {'loss': 0.4849, 'learning_rate': 4.103e-05, 'epoch': 8.97, 'step': 897},\n",
       " {'loss': 0.0007,\n",
       "  'learning_rate': 4.1020000000000004e-05,\n",
       "  'epoch': 8.98,\n",
       "  'step': 898},\n",
       " {'loss': 0.0182, 'learning_rate': 4.101e-05, 'epoch': 8.99, 'step': 899},\n",
       " {'loss': 0.0299, 'learning_rate': 4.1e-05, 'epoch': 9.0, 'step': 900},\n",
       " {'eval_loss': 1.5348423719406128,\n",
       "  'eval_accuracy': 0.56,\n",
       "  'eval_runtime': 5.8974,\n",
       "  'eval_samples_per_second': 16.957,\n",
       "  'eval_steps_per_second': 2.204,\n",
       "  'epoch': 9.0,\n",
       "  'step': 900},\n",
       " {'loss': 0.0005, 'learning_rate': 4.099e-05, 'epoch': 9.01, 'step': 901},\n",
       " {'loss': 0.2105,\n",
       "  'learning_rate': 4.0980000000000004e-05,\n",
       "  'epoch': 9.02,\n",
       "  'step': 902},\n",
       " {'loss': 0.0113, 'learning_rate': 4.097e-05, 'epoch': 9.03, 'step': 903},\n",
       " {'loss': 0.1669, 'learning_rate': 4.096e-05, 'epoch': 9.04, 'step': 904},\n",
       " {'loss': 0.1234, 'learning_rate': 4.095e-05, 'epoch': 9.05, 'step': 905},\n",
       " {'loss': 0.0076, 'learning_rate': 4.094e-05, 'epoch': 9.06, 'step': 906},\n",
       " {'loss': 0.001, 'learning_rate': 4.093e-05, 'epoch': 9.07, 'step': 907},\n",
       " {'loss': 0.1293, 'learning_rate': 4.092e-05, 'epoch': 9.08, 'step': 908},\n",
       " {'loss': 0.4484,\n",
       "  'learning_rate': 4.0910000000000006e-05,\n",
       "  'epoch': 9.09,\n",
       "  'step': 909},\n",
       " {'loss': 0.0065, 'learning_rate': 4.09e-05, 'epoch': 9.1, 'step': 910},\n",
       " {'loss': 0.0013, 'learning_rate': 4.089e-05, 'epoch': 9.11, 'step': 911},\n",
       " {'loss': 0.0066, 'learning_rate': 4.088e-05, 'epoch': 9.12, 'step': 912},\n",
       " {'loss': 0.0022, 'learning_rate': 4.087e-05, 'epoch': 9.13, 'step': 913},\n",
       " {'loss': 0.0877,\n",
       "  'learning_rate': 4.0860000000000005e-05,\n",
       "  'epoch': 9.14,\n",
       "  'step': 914},\n",
       " {'loss': 0.0549, 'learning_rate': 4.085e-05, 'epoch': 9.15, 'step': 915},\n",
       " {'loss': 0.0222, 'learning_rate': 4.084e-05, 'epoch': 9.16, 'step': 916},\n",
       " {'loss': 0.0103, 'learning_rate': 4.083e-05, 'epoch': 9.17, 'step': 917},\n",
       " {'loss': 0.0023,\n",
       "  'learning_rate': 4.0820000000000006e-05,\n",
       "  'epoch': 9.18,\n",
       "  'step': 918},\n",
       " {'loss': 0.001,\n",
       "  'learning_rate': 4.0810000000000004e-05,\n",
       "  'epoch': 9.19,\n",
       "  'step': 919},\n",
       " {'loss': 0.0003, 'learning_rate': 4.08e-05, 'epoch': 9.2, 'step': 920},\n",
       " {'loss': 0.0084, 'learning_rate': 4.079e-05, 'epoch': 9.21, 'step': 921},\n",
       " {'loss': 0.0348, 'learning_rate': 4.078e-05, 'epoch': 9.22, 'step': 922},\n",
       " {'loss': 0.0001,\n",
       "  'learning_rate': 4.0770000000000004e-05,\n",
       "  'epoch': 9.23,\n",
       "  'step': 923},\n",
       " {'loss': 0.0007, 'learning_rate': 4.076e-05, 'epoch': 9.24, 'step': 924},\n",
       " {'loss': 0.0351, 'learning_rate': 4.075e-05, 'epoch': 9.25, 'step': 925},\n",
       " {'loss': 0.0176, 'learning_rate': 4.074e-05, 'epoch': 9.26, 'step': 926},\n",
       " {'loss': 0.0003,\n",
       "  'learning_rate': 4.0730000000000005e-05,\n",
       "  'epoch': 9.27,\n",
       "  'step': 927},\n",
       " {'loss': 0.0004, 'learning_rate': 4.072e-05, 'epoch': 9.28, 'step': 928},\n",
       " {'loss': 0.0005, 'learning_rate': 4.071e-05, 'epoch': 9.29, 'step': 929},\n",
       " {'loss': 0.0009, 'learning_rate': 4.07e-05, 'epoch': 9.3, 'step': 930},\n",
       " {'loss': 0.0005, 'learning_rate': 4.069e-05, 'epoch': 9.31, 'step': 931},\n",
       " {'loss': 0.024,\n",
       "  'learning_rate': 4.0680000000000004e-05,\n",
       "  'epoch': 9.32,\n",
       "  'step': 932},\n",
       " {'loss': 0.0, 'learning_rate': 4.067e-05, 'epoch': 9.33, 'step': 933},\n",
       " {'loss': 0.0002, 'learning_rate': 4.066e-05, 'epoch': 9.34, 'step': 934},\n",
       " {'loss': 0.0013, 'learning_rate': 4.065e-05, 'epoch': 9.35, 'step': 935},\n",
       " {'loss': 0.0007, 'learning_rate': 4.064e-05, 'epoch': 9.36, 'step': 936},\n",
       " {'loss': 0.5236, 'learning_rate': 4.063e-05, 'epoch': 9.37, 'step': 937},\n",
       " {'loss': 0.0014, 'learning_rate': 4.062e-05, 'epoch': 9.38, 'step': 938},\n",
       " {'loss': 0.0042,\n",
       "  'learning_rate': 4.0610000000000006e-05,\n",
       "  'epoch': 9.39,\n",
       "  'step': 939},\n",
       " {'loss': 0.0014,\n",
       "  'learning_rate': 4.0600000000000004e-05,\n",
       "  'epoch': 9.4,\n",
       "  'step': 940},\n",
       " {'loss': 0.0002, 'learning_rate': 4.059e-05, 'epoch': 9.41, 'step': 941},\n",
       " {'loss': 0.0002, 'learning_rate': 4.058e-05, 'epoch': 9.42, 'step': 942},\n",
       " {'loss': 0.0017, 'learning_rate': 4.057e-05, 'epoch': 9.43, 'step': 943},\n",
       " {'loss': 0.0021,\n",
       "  'learning_rate': 4.0560000000000005e-05,\n",
       "  'epoch': 9.44,\n",
       "  'step': 944},\n",
       " {'loss': 0.0008, 'learning_rate': 4.055e-05, 'epoch': 9.45, 'step': 945},\n",
       " {'loss': 0.0247, 'learning_rate': 4.054e-05, 'epoch': 9.46, 'step': 946},\n",
       " {'loss': 0.045, 'learning_rate': 4.053e-05, 'epoch': 9.47, 'step': 947},\n",
       " {'loss': 0.0859,\n",
       "  'learning_rate': 4.0520000000000005e-05,\n",
       "  'epoch': 9.48,\n",
       "  'step': 948},\n",
       " {'loss': 0.0,\n",
       "  'learning_rate': 4.0510000000000003e-05,\n",
       "  'epoch': 9.49,\n",
       "  'step': 949},\n",
       " {'loss': 0.0, 'learning_rate': 4.05e-05, 'epoch': 9.5, 'step': 950},\n",
       " {'loss': 0.0, 'learning_rate': 4.049e-05, 'epoch': 9.51, 'step': 951},\n",
       " {'loss': 0.0, 'learning_rate': 4.048e-05, 'epoch': 9.52, 'step': 952},\n",
       " {'loss': 0.5863,\n",
       "  'learning_rate': 4.0470000000000004e-05,\n",
       "  'epoch': 9.53,\n",
       "  'step': 953},\n",
       " {'loss': 0.0, 'learning_rate': 4.046e-05, 'epoch': 9.54, 'step': 954},\n",
       " {'loss': 0.0,\n",
       "  'learning_rate': 4.045000000000001e-05,\n",
       "  'epoch': 9.55,\n",
       "  'step': 955},\n",
       " {'loss': 0.0001, 'learning_rate': 4.044e-05, 'epoch': 9.56, 'step': 956},\n",
       " {'loss': 0.0,\n",
       "  'learning_rate': 4.0430000000000004e-05,\n",
       "  'epoch': 9.57,\n",
       "  'step': 957},\n",
       " {'loss': 0.0812, 'learning_rate': 4.042e-05, 'epoch': 9.58, 'step': 958},\n",
       " {'loss': 0.0075, 'learning_rate': 4.041e-05, 'epoch': 9.59, 'step': 959},\n",
       " {'loss': 1.7567,\n",
       "  'learning_rate': 4.0400000000000006e-05,\n",
       "  'epoch': 9.6,\n",
       "  'step': 960},\n",
       " {'loss': 0.0002, 'learning_rate': 4.039e-05, 'epoch': 9.61, 'step': 961},\n",
       " {'loss': 0.0047, 'learning_rate': 4.038e-05, 'epoch': 9.62, 'step': 962},\n",
       " {'loss': 0.0072, 'learning_rate': 4.037e-05, 'epoch': 9.63, 'step': 963},\n",
       " {'loss': 0.0011,\n",
       "  'learning_rate': 4.0360000000000007e-05,\n",
       "  'epoch': 9.64,\n",
       "  'step': 964},\n",
       " {'loss': 0.4502,\n",
       "  'learning_rate': 4.0350000000000005e-05,\n",
       "  'epoch': 9.65,\n",
       "  'step': 965},\n",
       " {'loss': 0.0197, 'learning_rate': 4.034e-05, 'epoch': 9.66, 'step': 966},\n",
       " {'loss': 1.2313, 'learning_rate': 4.033e-05, 'epoch': 9.67, 'step': 967},\n",
       " {'loss': 0.0085, 'learning_rate': 4.032e-05, 'epoch': 9.68, 'step': 968},\n",
       " {'loss': 0.0046,\n",
       "  'learning_rate': 4.0310000000000005e-05,\n",
       "  'epoch': 9.69,\n",
       "  'step': 969},\n",
       " {'loss': 0.0133,\n",
       "  'learning_rate': 4.0300000000000004e-05,\n",
       "  'epoch': 9.7,\n",
       "  'step': 970},\n",
       " {'loss': 0.0012, 'learning_rate': 4.029e-05, 'epoch': 9.71, 'step': 971},\n",
       " {'loss': 0.0043, 'learning_rate': 4.028e-05, 'epoch': 9.72, 'step': 972},\n",
       " {'loss': 0.0013, 'learning_rate': 4.027e-05, 'epoch': 9.73, 'step': 973},\n",
       " {'loss': 0.2741,\n",
       "  'learning_rate': 4.0260000000000004e-05,\n",
       "  'epoch': 9.74,\n",
       "  'step': 974},\n",
       " {'loss': 0.0086, 'learning_rate': 4.025e-05, 'epoch': 9.75, 'step': 975},\n",
       " {'loss': 0.0283, 'learning_rate': 4.024e-05, 'epoch': 9.76, 'step': 976},\n",
       " {'loss': 0.0005, 'learning_rate': 4.023e-05, 'epoch': 9.77, 'step': 977},\n",
       " {'loss': 0.0869,\n",
       "  'learning_rate': 4.0220000000000005e-05,\n",
       "  'epoch': 9.78,\n",
       "  'step': 978},\n",
       " {'loss': 0.1399, 'learning_rate': 4.021e-05, 'epoch': 9.79, 'step': 979},\n",
       " {'loss': 0.0022, 'learning_rate': 4.02e-05, 'epoch': 9.8, 'step': 980},\n",
       " {'loss': 0.0009, 'learning_rate': 4.019e-05, 'epoch': 9.81, 'step': 981},\n",
       " {'loss': 0.0017, 'learning_rate': 4.018e-05, 'epoch': 9.82, 'step': 982},\n",
       " {'loss': 0.0021, 'learning_rate': 4.017e-05, 'epoch': 9.83, 'step': 983},\n",
       " {'loss': 0.0001, 'learning_rate': 4.016e-05, 'epoch': 9.84, 'step': 984},\n",
       " {'loss': 0.0002,\n",
       "  'learning_rate': 4.015000000000001e-05,\n",
       "  'epoch': 9.85,\n",
       "  'step': 985},\n",
       " {'loss': 0.0016, 'learning_rate': 4.014e-05, 'epoch': 9.86, 'step': 986},\n",
       " {'loss': 0.0003,\n",
       "  'learning_rate': 4.0130000000000004e-05,\n",
       "  'epoch': 9.87,\n",
       "  'step': 987},\n",
       " {'loss': 0.0, 'learning_rate': 4.012e-05, 'epoch': 9.88, 'step': 988},\n",
       " {'loss': 0.0009, 'learning_rate': 4.011e-05, 'epoch': 9.89, 'step': 989},\n",
       " {'loss': 0.0,\n",
       "  'learning_rate': 4.0100000000000006e-05,\n",
       "  'epoch': 9.9,\n",
       "  'step': 990},\n",
       " {'loss': 0.0008, 'learning_rate': 4.009e-05, 'epoch': 9.91, 'step': 991},\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13febf82-bd2e-481a-a6d7-441bc0d83e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('reward_model_tokenizer_50epochs/tokenizer_config.json',\n",
       " 'reward_model_tokenizer_50epochs/special_tokens_map.json',\n",
       " 'reward_model_tokenizer_50epochs/vocab.txt',\n",
       " 'reward_model_tokenizer_50epochs/added_tokens.json',\n",
       " 'reward_model_tokenizer_50epochs/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"reward_mode_50epochs/\")\n",
    "tokenizer.save_pretrained(\"reward_model_tokenizer_50epochs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57e723fa-4675-4391-9370-bfc3d36026ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.442725896835327, 'eval_accuracy': 0.52, 'eval_runtime': 5.5011, 'eval_samples_per_second': 18.178, 'eval_steps_per_second': 2.363, 'epoch': 50.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print metrics\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f253380c-cc23-4023-afa5-03675fc51d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictions, label_ids, _ = trainer.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "395678c4-9658-40bb-9267-1eebca9b98e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "# Convert logits to labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Compute accuracy or any other metric\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(label_ids, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39a91c64-e139-4c4b-876f-1cb44f0c539a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd2b1baaf5543708da17b778bb6029e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset_for_accuracy = test_dataset.map(process_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e32740d6-013e-499c-be29-227fdfaeaeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "predictions, label_ids, _ = trainer.predict(test_dataset_for_accuracy)\n",
    "# Convert logits to labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Compute accuracy or any other metric\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(label_ids, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b44178c-ada3-4028-ad14-bf271629f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea5328dd-01cf-46a5-9497-fecaccb9c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def score_summaries(model, tokenizer, chosen_summary, rejected_summary):\n",
    "    # Tokenize the inputs\n",
    "    chosen_tokens = tokenizer(chosen_summary, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "    rejected_tokens = tokenizer(rejected_summary, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "    chosen_tokens.to(device)\n",
    "    rejected_tokens.to(device)\n",
    "\n",
    "\n",
    "    # Get logits from the model\n",
    "    with torch.no_grad():\n",
    "        chosen_logits = model(**chosen_tokens).logits\n",
    "        rejected_logits = model(**rejected_tokens).logits\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    chosen_probs = F.softmax(chosen_logits, dim=-1)\n",
    "    rejected_probs = F.softmax(rejected_logits, dim=-1)\n",
    "\n",
    "    # Assuming the positive class (indicating 'chosen' is good) is the second one\n",
    "    chosen_score = chosen_probs[0][1].item()\n",
    "    rejected_score = rejected_probs[0][1].item()\n",
    "\n",
    "    # Extract logits for each summary\n",
    "    chosen_logit = chosen_logits[0][1].item()\n",
    "    rejected_logit = rejected_logits[0][1].item()\n",
    "\n",
    "    return chosen_score, rejected_score, chosen_logit, rejected_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "432d316a-b3de-4747-b5b1-86c41777a23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Score: 0.5441\n",
      "Rejected Score: 0.4987\n",
      "Chosen Logit: 6.1556\n",
      "Rejected Logit: -6.4933\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "chosen_summary = \"TL;DR: Water meter in another condo is not in our condo. What can we do legally to restore water to my condo complex?\"\n",
    "rejected_summary = \"TL;DR: I don't know\"\n",
    "\n",
    "chosen_score, rejected_score, chosen_logit, rejected_logit = score_summaries(model, tokenizer, chosen_summary, rejected_summary)\n",
    "\n",
    "print(f\"Chosen Score: {chosen_score:.4f}\")\n",
    "print(f\"Rejected Score: {rejected_score:.4f}\")\n",
    "\n",
    "print(f\"Chosen Logit: {chosen_logit:.4f}\")\n",
    "print(f\"Rejected Logit: {rejected_logit:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77289b36-c383-4d21-972c-66603c933764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42a3acfc-dba8-4081-9506-1c1fd7af13e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TL;DR: My boyfriend cheated on me by having an intimate Skype convo with another girl over webcam after 4+ years together. I can't forgive him and don't feel the same way about him anymore.\n",
      "TL;DR: Girlfriend is having second thoughts about our relationship, and I'm worried that I'm setting myself up for pain. Am I setting myself up for pain?\n",
      "TL;DR: found out my boyfriend cheated on me over webcam with a girl he got off a webcam site.. I am having a lot of trouble forgiving him and I don't know what to do.\n",
      "TL;DR: The only good reason I can think of for humans to be in space is because we think it's important to say we are there.\n",
      "TL;DR: Ex of 7 months broke up with me, she apologized and asked if we could be friends, i agreed, things were great till last Thursday she began to ignore me, dont know why. Help pls\n",
      "TL;DR: I'm looking for ideas to propose to my boyfriend, and I don't know what to do to make it not awkward.\n",
      "TL;DR: boyfriend stopped having sex, says nothing is wrong, I'm not sure I believe him and I'm getting lonely and confused on how to fix this?\n",
      "TL;DR: my boyfriend of 3 years is now flirting with a new co-worker, is he having an emotional affair with her?\n",
      "TL;DR: Company pays me a percentage of net earnings, money goes missing, accounting department calls me constantly, company is involved in legal action with software company, not sure if I will ever recover any money. Help?\n",
      "TL;DR: broke up with my girlfriend of 1 year because she is a no life stoner and is causing a rift in our relationship.\n",
      "TL;DR: I'm looking for books that make me understand the psyche of a man, and are filled with what it feels like to be a man.\n",
      "TL;DR: My dad passed away yesterday, older sis doesn't seem to grieve. I'm annoyed by her lack of emotion and don't understand her response. How do I interpret her behavior?\n",
      "TL;DR: Brother got pulled out of a championship just because the other kid who was faster showed up--though was late. Is that right or fucked up?\n",
      "TL;DR: Told bf that I have to force myself to have sex with him. He was hurt by what I said. How do I make him feel better.\n",
      "TL;DR: Friend was a dick to me for applying for a summer job. I decided to be the dick to get the job. Friend got mad at me.\n",
      "TL;DR: Wife has been cheated on a lot in the past, asked me if I want to fuck other girls, I now do but I feel guilty because I'm her first nice/caring partner.\n",
      "TL;DR: met awesome girl, best of friends for about 2 years, told her if she wanted to go out with me, never responded, broken friendship, help me understand what could've happened to her?\n",
      "TL;DR: Been seeing a girl for 2 years. She's finally moving on and seeing someone else. I'm sad. Parents and family adored her. Need advice on to break the news and get back in the game.\n",
      "TL;DR: I want to be a mad scientist/engineer, but that's not what the world needs. Also, having six arms would be so fucking awesome to make up for the impracticality it would bring on.\n",
      "TL;DR: My teen campers might ask me about sex slang and I want to gain as much knowledge of current slang right now as possible.\n",
      "TL;DR: My long distance girlfriend broke up with me after 7 months. Should I be friends with her or go No Contact. I have a lot of feelings for her and want to be friends with her, but not doing that will hurt me.\n",
      "TL;DR: My boyfriend still has his parents to support him and I'm not sure whether or not to bring it up. I love him but am worried about his future.\n",
      "TL;DR: I'm struggling between taking my pet away from the people who are watching her (she loves it there) or keeping her with me in an apartment.\n",
      "TL;DR: my long distance gf is contemplating suicide, she doesn't want to talk about it, what should I do?\n",
      "TL;DR: caught boyfriend watching ex's porn with him on his laptop, he agrees it's over the line but thinks it's okay to watch it with him.\n",
      "TL;DR: Girl wants to be single after being in a previous bf. wants to be just friends w/o commitment. need advise on mindset and expectations going forward!\n",
      "TL;DR: Boyfriend let me win at arm wrestle contest with my friend, she said that it will hurt my confidence, is she telling the truth?\n",
      "TL;DR: I am married with a kid, hooked up with a girl in Asia, but remain in contact, and am writing a lot of emails. Is this cheating ?\n",
      "TL;DR: two women exchange witty messages on POF, meet up irl,  he tries to help me buy a smart phone and a year of service on virgin mobile, i need to do it before i get dependent on him.  what do i do?\n",
      "TL;DR: Girlfriend of 1 year dumped me, and I'm just confused and heart broken. I want to be friends so I can prove myself to her, but I'm not sure if I can be since she's moved on.\n",
      "TL;DR: Presence of choke chain makes walks much easier without ever choking, harming or stressing out my dog, but is it bringing back bad memories from his previous owners?\n",
      "TL;DR: BebopVox covered Minecraft convention scam, what are your stories about becoming victims of Conventions and Meetups? What advice would you give to those who have gone through this?\n",
      "TL;DR: Thanks to my ex, everytime the guy I am dating says he did something with \"a friend\" from work, a trigger goes off that he must be sleeping with them.\n",
      "TL;DR: Bf got me promise ring, now I'm not crazy about it but I don't want to lose it.\n",
      "TL;DR: Parents suddenly no longer work for employer providing my family health benefits. Need to know how to choose a health care plan to provide for my family.\n",
      "TL;DR: I'm turning 30 tomorrow, and I dread it. I'm going to show up to my house and have a drink, watch a movie, have some burgers with my friends, and just generally relax.\n",
      "TL;DR: I'm scared to make any moves even though she wants it and she wants a relationship. How do i ask her out?\n",
      "TL;DR: Can a choke chain cause psychological damage to a dog that has been treated badly in the past? Will it cause him to bring up bad memories?\n",
      "TL;DR: Recent grad in Management Information Systems needs 2-10 years experience to get a job in any related field. No direction, no jobs. Don't get an MIS major.\n",
      "TL;DR: Every single one of my exes is married, and all of the ones that had interest, have kids. I'm tired of constantly being the last guy people are with.\n",
      "TL;DR: I didn't say sorry when I was late to pick him up, making him wait in the cold for 20 minutes. He was mad at me for it. Says it's common sense to apologize.\n",
      "TL;DR: I got back together with my ex, but it is not the same.  I don't know if I'm over-reacting or if I'm just fooling myself.\n",
      "TL;DR: ex of four years wants me to sign as guarantor on property let, I don't want to but feeling pressured to do so. Am I right to stick to my guns?\n",
      "TL;DR: If you go to  and click 'voter' (French for vote), you can help send me to space.\n",
      "TL;DR: I received a check from my girlfriend's parents for graduating and I'm not sure how appropriate it is to accept it.\n",
      "TL;DR: Born an extrovert. Bullied into being an introvert. Shit sucked, but I got over it. Gaining confidence back, slowly though. All advice on that and meeting new people appreciated.\n",
      "TL;DR: Married 8 yrs, husband doesn't seem to be interested in me sexually anymore. I have tried to talk to him about it but he doesn't see it as a problem? Any advice on how to get the spark back?\n",
      "TL;DR: Had retail store manager job, had trouble staying at job due to opiate addiction, worked in finance from 2009 until last month. Want to show job history in best light. What should I focus on?\n",
      "TL;DR: Two teens interested in each other, but still nothing is happening, should I even bother? I don't need immediate or exact answers, only reason why I'm posting this is to get some more POVs about the subject.\n",
      "TL;DR: I like this girl. She is moving back to her own country in couple of weeks. How can I tell her that I want to have a long term relationship without freaking her out and coming off needy or creepy?\n",
      "TL;DR: Girl is very conscious about herself and how she looks. She naturally looks good, but how can I tell it to her so she feels good about herself?\n",
      "TL;DR: Im always the one who breaks off the realtionship.  And am beginning to think that somethings wrong with me for never really falling for girls the way they seem to be falling for me.\n",
      "TL;DR: my friend's boyfriend refuses to even talk about settling down after 4 years together, 2 1/2 living together. What advice should I give her? He's a great boyfriend otherwise, but I feel he's stringing her along.\n",
      "TL;DR: I am a crappy computer user and broke a computer at my library. After lots of research, I fixed it and installed a new circulation system on my boss's computer.\n",
      "TL;DR: I took off my clothes to cool off after a bike ride and my friend thought I was fapping to Starcraft 2.\n",
      "TL;DR: African dude on special project not doing his part, I told him to slow down and do his job right, got fired. What a fucking waste of time.\n",
      "TL;DR: wants to buy me a phone.  i don't want to see it as a way to feel obligated to give sexy times.  he says it's a NSA gift.  i said i'd consult reddit.  do i take it?\n",
      "TL;DR: Parents want to host Thanksgiving dinner despite long distance and limited time to prepare due to serious illness/move. Husband is pissed at me for suggesting otherwise. What do?\n",
      "TL;DR: My girlfriend hates my daughter's mother (my ex who cheated on me), and I need advice on how to help her cope.\n",
      "TL;DR: Shy guy wants to meet cute girl of interest but school is ending and can only see her in school mornings, need help in approaching her and introducing myself/getting the courage to talk to her\n",
      "TL;DR: I'm fat, I want to lose weight, but don't want to do a lot of muscle building, where do I start?\n",
      "TL;DR: Best friend of 7 years is going through a marriage/divorce with a woman he married and is now getting an annulment because she's a bi/lesbian.  He's heartbroken and feels like he wishes he could have her back.  How can I help him understand that although he did bad things and wasn't the best boyfriend/fiance in the world, it's still not his fault and he shouldn't feel like he wishes he could have her back but without all this crap?\n",
      "TL;DR: SO is late every time we have plans, just doesn't bother to contact me or make me aware of it. I'm feeling shitty and feel like he doesn't care about me when he makes me late. What do I do?\n",
      "TL;DR: For the past 8 out of 10 days girlfriend of 4 months has been drunk with friends. I don't feel great about that, but not because of lack of trust. Is there a best way to voice this to her or am I just being overbearing?\n",
      "TL;DR: Was promised something for free, bought it anyway and gave it back so the next person told what I was told will actually get something for free.\n",
      "TL;DR: My sister got engaged 4 months after I got engaged and I feel like they only got engaged to one up us and because we got engaged.\n",
      "TL;DR: ordered tzatziki at Greek shop in Boston, everyone around me looked disgusted/bewildered when I ate it due to ingredient list. *side note* anyone have experience with the actual product?\n",
      "TL;DR: Should I let my husband try to find himself without me or should I go on alone? I think the odds of him coming back are slim.\n",
      "TL;DR: Been seeing each other for 6 months, think relationship going stale during uni weeks/days. Want advise/insight as to why/how to fix/not fall out of love w/ her :( Help pls. pls!\n",
      "TL;DR: I cannot seem to pick up the hints my girlfriend sends me. I have done everything asked of me and I still can't.\n",
      "TL;DR: been talking to girl for 1.5 months, father has heart attack; unsure of next move; she still wants to continue talking; am I wasting too much time over this???\n",
      "TL;DR: Gf of 2 1/2 years dumped me for the 2nd time5 months ago, kept in contact and agreed to take things slow. Should I even bother?\n",
      "TL;DR: been with gf for 2 years, on and off with ex, gonna hook up with old friend, herpes is involved, what do i do?\n",
      "TL;DR: never been kissed, never been asked out romantically, bad vibes from romantic standpoint, can't seem to find anyone who's interested, coming to realize I'll be forever alone, any advice on how to be happy with that knowledge?\n",
      "TL;DR: Boyfriend of 9 months is very busy and stressed and isn't ready to commit to a serious relationship yet. We are fighting a lot over small things and I'm feeling insecure and generally unhappy with our relationship. What should I do?\n",
      "TL;DR: Been talking to this girl for a while, she says she wants me to ask her out, how do i ask her out to be my girlfriend and should i say \"will you go out with me\" or something?\n",
      "TL;DR: Took ex out to dinner, we kissed, she asked me to pick her up from her ex's place, I take her back to mine, mixed feelings, hate and desire.\n",
      "TL;DR: living with passive agressive gross housemate that leaves her used tampons in the bowl and when I confront her she gets pissed at me.  How do I protect myself from her bad behavior?\n",
      "TL;DR: My husband is flirty with his female friends and I'm uncomfortable with it. He doesn't think anything is wrong. What can I do?\n",
      "TL;DR: My dad doesn't seem to care about me. I'm in college but I have no idea what to do, or if I'm even going to be able to move out anytime soon.\n",
      "TL;DR: Girl says she likes me, but then says shes in my class and then says she cant hang out with me.\n",
      "TL;DR: friend got angry cause region locked DLC passport for online play was region locked, tried to exchange it and played it like there was no rage last night. what's the most ungrateful thing you ever see a person do ?\n",
      "TL;DR: Crush with long term schoolmate for over 8 years. Gave me her number a while ago, never hung out, going to party with friend there. Do you think there is a chance?\n",
      "TL;DR: I (19M) wanted to ask out friend of couple months (20M) only to find out his ex (20M) is moving in.\n",
      "TL;DR: I was a dick and didn't share my cookie with the server who shared her food with me.\n",
      "TL;DR: My dad is being a jerk about my engagement. I'm hurt and angry and don't know what to do. I'm fed up with his crap.\n",
      "TL;DR: I finally got a job after being denied many times over the past year after applying to job sites and temp agencies. I wish to thank reddit for all of the tips that I was able to get out of all of this.\n",
      "TL;DR: I never dated/flirted as an adult, now I'm not sure how to date. Scared will grow old with many cats. Any advice?\n",
      "TL;DR: Brother's Ex-GF left him because she was seeing another guy when they were in different colleges. He later finds out her \"friend\" that she was seeing was actually Jake. What should he do?\n",
      "TL;DR: Long distance boyfriend of a few months doesn't seem to want to talk to me and responds with short one or two word answers. How do I nicely ask him to change this behavior?\n",
      "TL;DR: I [21M] think I'm a bad, egocentric, narcissistic, person, but my friends and family think I'm a great person. I feel angry towards them\n",
      "TL;DR: Met a guy, he wasn't into LDR's, he wanted to try it for me, I'm considering it for myself and in the summer we'll meet.\n",
      "TL;DR: My friend's sister is obviously being scammed. What do we do? Is there any precedent for bringing legal proceedings (or anything else) against someone like this?\n",
      "TL;DR: I have had dreams about a girl I barely know for 8 years, after which I am currently in a relationship and know that this other girl is single. Still fantasize on her. Help?\n",
      "TL;DR: Was 7, went to the bathroom, tried to piss and ended up hitting the urinal and pissing in my eye.\n",
      "TL;DR: I just broke up with my GF and I'm not over her.\n",
      "TL;DR: My teen campers might ask me about sex slang and I want to gain as much knowledge of current slang right now as possible.\n",
      "TL;DR: My ex turned down a job opportunity because of me, how do I make sure he doesn't do something stupid like that again without breaking the no contact rule I put in place?\n",
      "TL;DR: Girlfriend compared my come to her ex's and doesn't understand why that upset me. It doesn't seem to bother her and I don't know if I'm overreacting or if it should bother me.\n",
      "TL;DR: My friend's irresponsible mom wants to watch my puppy when it's just my boyfriend and I, I don't feel safe with her watching him. What do I do?\n"
     ]
    }
   ],
   "source": [
    "for i in test_dataset['chosen']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e69317d-7dbb-47bf-9dff-32ed75aefe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Score: 0.5618\n",
      "Rejected Score: 0.5136\n",
      "Chosen Logit: 6.2911\n",
      "Rejected Logit: -5.9902\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4982\n",
      "Rejected Score: 0.4950\n",
      "Chosen Logit: -5.8289\n",
      "Rejected Logit: -1.4790\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4849\n",
      "Rejected Score: 0.4907\n",
      "Chosen Logit: 1.3715\n",
      "Rejected Logit: -6.4591\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4962\n",
      "Rejected Score: 0.4974\n",
      "Chosen Logit: -6.4856\n",
      "Rejected Logit: -5.4297\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5636\n",
      "Rejected Score: 0.5458\n",
      "Chosen Logit: 1.3878\n",
      "Rejected Logit: -1.2086\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5298\n",
      "Rejected Score: 0.5078\n",
      "Chosen Logit: 1.3085\n",
      "Rejected Logit: -6.5759\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4918\n",
      "Rejected Score: 0.5505\n",
      "Chosen Logit: -3.2979\n",
      "Rejected Logit: -2.2035\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5141\n",
      "Rejected Score: 0.4688\n",
      "Chosen Logit: 6.0325\n",
      "Rejected Logit: 1.6186\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5646\n",
      "Rejected Score: 0.5182\n",
      "Chosen Logit: 6.5027\n",
      "Rejected Logit: -4.5801\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5215\n",
      "Rejected Score: 0.5338\n",
      "Chosen Logit: 1.8814\n",
      "Rejected Logit: 1.1466\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5360\n",
      "Rejected Score: 0.5515\n",
      "Chosen Logit: 3.3674\n",
      "Rejected Logit: 7.9164\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5052\n",
      "Rejected Score: 0.5132\n",
      "Chosen Logit: -5.8913\n",
      "Rejected Logit: -5.6305\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5537\n",
      "Rejected Score: 0.5541\n",
      "Chosen Logit: 3.3181\n",
      "Rejected Logit: 2.3309\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5047\n",
      "Rejected Score: 0.5095\n",
      "Chosen Logit: -6.0403\n",
      "Rejected Logit: -5.1451\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4981\n",
      "Rejected Score: 0.5329\n",
      "Chosen Logit: -4.3568\n",
      "Rejected Logit: -3.8980\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5181\n",
      "Rejected Score: 0.4956\n",
      "Chosen Logit: 0.1718\n",
      "Rejected Logit: -6.1066\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5109\n",
      "Rejected Score: 0.5428\n",
      "Chosen Logit: -4.3355\n",
      "Rejected Logit: -0.3854\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5765\n",
      "Rejected Score: 0.5328\n",
      "Chosen Logit: -2.8142\n",
      "Rejected Logit: -4.7935\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4972\n",
      "Rejected Score: 0.4797\n",
      "Chosen Logit: -5.7228\n",
      "Rejected Logit: -6.3522\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5465\n",
      "Rejected Score: 0.5126\n",
      "Chosen Logit: 5.5598\n",
      "Rejected Logit: -5.0993\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4828\n",
      "Rejected Score: 0.5212\n",
      "Chosen Logit: 1.4421\n",
      "Rejected Logit: -1.5677\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5379\n",
      "Rejected Score: 0.5059\n",
      "Chosen Logit: 7.9549\n",
      "Rejected Logit: -3.1919\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4873\n",
      "Rejected Score: 0.5273\n",
      "Chosen Logit: 0.1984\n",
      "Rejected Logit: -4.8920\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4919\n",
      "Rejected Score: 0.5092\n",
      "Chosen Logit: -7.2955\n",
      "Rejected Logit: -0.5604\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4940\n",
      "Rejected Score: 0.5145\n",
      "Chosen Logit: -5.5282\n",
      "Rejected Logit: -1.4887\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5379\n",
      "Rejected Score: 0.5279\n",
      "Chosen Logit: 0.1668\n",
      "Rejected Logit: -0.0781\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5302\n",
      "Rejected Score: 0.5079\n",
      "Chosen Logit: 1.2459\n",
      "Rejected Logit: -4.2020\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5489\n",
      "Rejected Score: 0.5120\n",
      "Chosen Logit: 6.4512\n",
      "Rejected Logit: -6.3346\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5055\n",
      "Rejected Score: 0.5662\n",
      "Chosen Logit: -2.5378\n",
      "Rejected Logit: 6.5350\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5519\n",
      "Rejected Score: 0.5389\n",
      "Chosen Logit: 3.0943\n",
      "Rejected Logit: 4.2693\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5330\n",
      "Rejected Score: 0.5470\n",
      "Chosen Logit: 1.6189\n",
      "Rejected Logit: 3.8062\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5198\n",
      "Rejected Score: 0.5338\n",
      "Chosen Logit: -5.2950\n",
      "Rejected Logit: 0.0686\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5483\n",
      "Rejected Score: 0.5156\n",
      "Chosen Logit: 7.6787\n",
      "Rejected Logit: -5.5452\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4982\n",
      "Rejected Score: 0.5188\n",
      "Chosen Logit: -6.7370\n",
      "Rejected Logit: -5.3354\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5646\n",
      "Rejected Score: 0.5331\n",
      "Chosen Logit: 5.9167\n",
      "Rejected Logit: -3.7450\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5230\n",
      "Rejected Score: 0.5073\n",
      "Chosen Logit: -2.8389\n",
      "Rejected Logit: -5.8302\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4913\n",
      "Rejected Score: 0.4868\n",
      "Chosen Logit: -7.1904\n",
      "Rejected Logit: -5.5736\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4958\n",
      "Rejected Score: 0.5022\n",
      "Chosen Logit: -6.7818\n",
      "Rejected Logit: -6.1539\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5229\n",
      "Rejected Score: 0.5160\n",
      "Chosen Logit: -4.1172\n",
      "Rejected Logit: -2.0669\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4940\n",
      "Rejected Score: 0.5356\n",
      "Chosen Logit: -2.9385\n",
      "Rejected Logit: -1.7960\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4861\n",
      "Rejected Score: 0.5215\n",
      "Chosen Logit: -7.6260\n",
      "Rejected Logit: -4.4214\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5369\n",
      "Rejected Score: 0.5934\n",
      "Chosen Logit: 3.1189\n",
      "Rejected Logit: 7.0284\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5774\n",
      "Rejected Score: 0.5478\n",
      "Chosen Logit: 7.1252\n",
      "Rejected Logit: 7.5118\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5371\n",
      "Rejected Score: 0.5286\n",
      "Chosen Logit: 2.8583\n",
      "Rejected Logit: -4.8586\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4918\n",
      "Rejected Score: 0.5380\n",
      "Chosen Logit: -4.5386\n",
      "Rejected Logit: 7.9670\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5120\n",
      "Rejected Score: 0.5166\n",
      "Chosen Logit: -4.7131\n",
      "Rejected Logit: -5.0800\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4980\n",
      "Rejected Score: 0.5164\n",
      "Chosen Logit: -2.9656\n",
      "Rejected Logit: -4.9185\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5353\n",
      "Rejected Score: 0.5328\n",
      "Chosen Logit: 1.6436\n",
      "Rejected Logit: 1.7957\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5534\n",
      "Rejected Score: 0.5575\n",
      "Chosen Logit: 4.5520\n",
      "Rejected Logit: 6.1513\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5305\n",
      "Rejected Score: 0.5286\n",
      "Chosen Logit: 6.4916\n",
      "Rejected Logit: 4.8076\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5370\n",
      "Rejected Score: 0.5539\n",
      "Chosen Logit: 4.9587\n",
      "Rejected Logit: 1.9680\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4933\n",
      "Rejected Score: 0.5043\n",
      "Chosen Logit: -7.1103\n",
      "Rejected Logit: -5.2783\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5299\n",
      "Rejected Score: 0.5284\n",
      "Chosen Logit: -3.7087\n",
      "Rejected Logit: 4.8299\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5826\n",
      "Rejected Score: 0.5286\n",
      "Chosen Logit: -2.5339\n",
      "Rejected Logit: 1.8521\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5791\n",
      "Rejected Score: 0.5473\n",
      "Chosen Logit: 7.4382\n",
      "Rejected Logit: 0.9510\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5328\n",
      "Rejected Score: 0.5328\n",
      "Chosen Logit: -1.7317\n",
      "Rejected Logit: -0.1081\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5253\n",
      "Rejected Score: 0.5055\n",
      "Chosen Logit: -2.6538\n",
      "Rejected Logit: -2.5378\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5576\n",
      "Rejected Score: 0.5730\n",
      "Chosen Logit: -0.4315\n",
      "Rejected Logit: 4.1179\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5249\n",
      "Rejected Score: 0.5053\n",
      "Chosen Logit: -3.4647\n",
      "Rejected Logit: -4.0678\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5267\n",
      "Rejected Score: 0.5025\n",
      "Chosen Logit: 7.0822\n",
      "Rejected Logit: -5.6713\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5246\n",
      "Rejected Score: 0.5165\n",
      "Chosen Logit: 4.5102\n",
      "Rejected Logit: -4.7368\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5314\n",
      "Rejected Score: 0.5392\n",
      "Chosen Logit: 5.7771\n",
      "Rejected Logit: 6.3425\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5198\n",
      "Rejected Score: 0.5072\n",
      "Chosen Logit: 0.4331\n",
      "Rejected Logit: -3.5907\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5319\n",
      "Rejected Score: 0.4875\n",
      "Chosen Logit: 8.0136\n",
      "Rejected Logit: -4.4521\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5339\n",
      "Rejected Score: 0.5633\n",
      "Chosen Logit: 0.3256\n",
      "Rejected Logit: -0.6114\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5499\n",
      "Rejected Score: 0.5120\n",
      "Chosen Logit: -0.4777\n",
      "Rejected Logit: -1.1338\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5461\n",
      "Rejected Score: 0.5696\n",
      "Chosen Logit: -3.9030\n",
      "Rejected Logit: 1.4205\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5077\n",
      "Rejected Score: 0.5704\n",
      "Chosen Logit: -3.5655\n",
      "Rejected Logit: 5.3057\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5054\n",
      "Rejected Score: 0.5109\n",
      "Chosen Logit: -5.1296\n",
      "Rejected Logit: -5.6378\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5360\n",
      "Rejected Score: 0.5314\n",
      "Chosen Logit: 4.2529\n",
      "Rejected Logit: -2.2712\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5423\n",
      "Rejected Score: 0.5318\n",
      "Chosen Logit: 6.7003\n",
      "Rejected Logit: 0.4044\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5203\n",
      "Rejected Score: 0.5425\n",
      "Chosen Logit: -1.4418\n",
      "Rejected Logit: 8.1565\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5169\n",
      "Rejected Score: 0.5111\n",
      "Chosen Logit: 1.6916\n",
      "Rejected Logit: -5.0676\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5530\n",
      "Rejected Score: 0.5280\n",
      "Chosen Logit: 7.6827\n",
      "Rejected Logit: -0.8746\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4988\n",
      "Rejected Score: 0.5096\n",
      "Chosen Logit: -6.3307\n",
      "Rejected Logit: -5.4056\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5705\n",
      "Rejected Score: 0.5064\n",
      "Chosen Logit: 7.0729\n",
      "Rejected Logit: -6.3649\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5377\n",
      "Rejected Score: 0.5094\n",
      "Chosen Logit: 6.0020\n",
      "Rejected Logit: -5.5241\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5588\n",
      "Rejected Score: 0.5107\n",
      "Chosen Logit: 7.0600\n",
      "Rejected Logit: -6.4160\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4970\n",
      "Rejected Score: 0.5127\n",
      "Chosen Logit: -3.3279\n",
      "Rejected Logit: -6.5678\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5020\n",
      "Rejected Score: 0.4963\n",
      "Chosen Logit: 0.8757\n",
      "Rejected Logit: -3.5378\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5295\n",
      "Rejected Score: 0.5355\n",
      "Chosen Logit: 1.2669\n",
      "Rejected Logit: 8.0200\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5942\n",
      "Rejected Score: 0.5853\n",
      "Chosen Logit: 5.5963\n",
      "Rejected Logit: 7.4086\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5348\n",
      "Rejected Score: 0.5519\n",
      "Chosen Logit: 7.7971\n",
      "Rejected Logit: 7.4003\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5799\n",
      "Rejected Score: 0.5880\n",
      "Chosen Logit: 6.0655\n",
      "Rejected Logit: 6.4219\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5341\n",
      "Rejected Score: 0.5463\n",
      "Chosen Logit: -4.2588\n",
      "Rejected Logit: 6.9853\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5254\n",
      "Rejected Score: 0.5111\n",
      "Chosen Logit: -4.9345\n",
      "Rejected Logit: -3.8808\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5128\n",
      "Rejected Score: 0.5548\n",
      "Chosen Logit: -5.2525\n",
      "Rejected Logit: 3.8196\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5226\n",
      "Rejected Score: 0.5258\n",
      "Chosen Logit: 6.5363\n",
      "Rejected Logit: 4.6580\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5204\n",
      "Rejected Score: 0.5261\n",
      "Chosen Logit: -2.1224\n",
      "Rejected Logit: 8.1332\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5012\n",
      "Rejected Score: 0.5471\n",
      "Chosen Logit: -5.7081\n",
      "Rejected Logit: 7.8229\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5001\n",
      "Rejected Score: 0.4932\n",
      "Chosen Logit: -7.0640\n",
      "Rejected Logit: -7.2189\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5150\n",
      "Rejected Score: 0.5119\n",
      "Chosen Logit: -5.6403\n",
      "Rejected Logit: 1.8006\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5032\n",
      "Rejected Score: 0.5438\n",
      "Chosen Logit: 0.3665\n",
      "Rejected Logit: 4.6527\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5767\n",
      "Rejected Score: 0.5545\n",
      "Chosen Logit: 6.4460\n",
      "Rejected Logit: 2.6817\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5398\n",
      "Rejected Score: 0.5294\n",
      "Chosen Logit: -0.7718\n",
      "Rejected Logit: 4.1295\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.4947\n",
      "Rejected Score: 0.4957\n",
      "Chosen Logit: -7.3438\n",
      "Rejected Logit: -5.7337\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5465\n",
      "Rejected Score: 0.5148\n",
      "Chosen Logit: 5.5598\n",
      "Rejected Logit: -5.0451\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5224\n",
      "Rejected Score: 0.5130\n",
      "Chosen Logit: -4.6138\n",
      "Rejected Logit: 0.9245\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5276\n",
      "Rejected Score: 0.4941\n",
      "Chosen Logit: 8.1596\n",
      "Rejected Logit: -6.8207\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chosen Score: 0.5180\n",
      "Rejected Score: 0.5731\n",
      "Chosen Logit: 6.3982\n",
      "Rejected Logit: 4.8552\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chosen_summary = []\n",
    "rejected_summary = []\n",
    "for i,j in zip(test_dataset['chosen'], test_dataset['rejected']):\n",
    "    chosen_summary.append(i)\n",
    "    rejected_summary.append(j)\n",
    "\n",
    "for chosen, rejected in zip(chosen_summary, rejected_summary):\n",
    "    chosen_score, rejected_score, chosen_logit, rejected_logit = score_summaries(model, tokenizer, chosen, rejected)\n",
    "\n",
    "    print(f\"Chosen Score: {chosen_score:.4f}\")\n",
    "    print(f\"Rejected Score: {rejected_score:.4f}\")\n",
    "\n",
    "    print(f\"Chosen Logit: {chosen_logit:.4f}\")\n",
    "    print(f\"Rejected Logit: {rejected_logit:.4f}\")\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8011f9c-54b1-47d4-9a91-0f51f6fcd66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chosen_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc2348-8822-4a17-8586-de2bf0f0092e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
